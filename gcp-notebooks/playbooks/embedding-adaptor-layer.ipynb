{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f809d4de-d111-4f50-adfe-b85601c15f1f",
   "metadata": {},
   "source": [
    "# Embedding Adaptor Playbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c10794-7dac-40f7-a828-04fac730725e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.9.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: annotated-types==0.6.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: anyio==3.7.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
      "Requirement already satisfied: astroid==3.0.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: attrs==23.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (23.1.0)\n",
      "Requirement already satisfied: cachetools==5.3.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (5.3.2)\n",
      "Requirement already satisfied: certifi==2023.11.17 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (8.1.7)\n",
      "Requirement already satisfied: dataclasses-json==0.6.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.6.3)\n",
      "Requirement already satisfied: dill==0.3.7 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.3.7)\n",
      "Requirement already satisfied: faiss-cpu==1.7.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.7.4)\n",
      "Requirement already satisfied: fastapi==0.105.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.105.0)\n",
      "Requirement already satisfied: frozenlist==1.4.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.4.1)\n",
      "Requirement already satisfied: google-api-core==2.15.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (2.15.0)\n",
      "Requirement already satisfied: google-auth==2.25.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (2.25.2)\n",
      "Requirement already satisfied: google-cloud-aiplatform==1.43.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-bigquery==3.14.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (3.14.1)\n",
      "Requirement already satisfied: google-cloud-core==2.4.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager==1.11.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (1.11.0)\n",
      "Requirement already satisfied: google-cloud-storage==2.14.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (2.14.0)\n",
      "Requirement already satisfied: google-crc32c==1.5.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (1.5.0)\n",
      "Requirement already satisfied: google-resumable-media==2.7.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (2.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos==1.62.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (1.62.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1==0.13.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (0.13.0)\n",
      "Requirement already satisfied: grpcio==1.60.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (1.60.0)\n",
      "Requirement already satisfied: grpcio-status==1.60.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (1.60.0)\n",
      "Requirement already satisfied: gunicorn==21.2.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (21.2.0)\n",
      "Requirement already satisfied: h11==0.14.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (0.14.0)\n",
      "Requirement already satisfied: idna==3.6 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (3.6)\n",
      "Requirement already satisfied: isort==5.13.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (5.13.2)\n",
      "Requirement already satisfied: jsonpatch==1.33 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (1.33)\n",
      "Requirement already satisfied: jsonpointer==2.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (2.4)\n",
      "Requirement already satisfied: langchain==0.1.10 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (0.1.10)\n",
      "Requirement already satisfied: langchain-community==0.0.25 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 36)) (0.0.25)\n",
      "Requirement already satisfied: langchain-core==0.1.28 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 37)) (0.1.28)\n",
      "Requirement already satisfied: langchain-google-vertexai==0.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (0.1.0)\n",
      "Requirement already satisfied: langchain-text-splitters==0.0.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (0.0.1)\n",
      "Requirement already satisfied: langsmith==0.1.10 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 40)) (0.1.10)\n",
      "Requirement already satisfied: marshmallow==3.20.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (3.20.1)\n",
      "Requirement already satisfied: mccabe==0.7.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (0.7.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (6.0.4)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (1.0.0)\n",
      "Requirement already satisfied: numpy==1.26.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (1.26.2)\n",
      "Requirement already satisfied: orjson==3.9.15 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (3.9.15)\n",
      "Requirement already satisfied: packaging==23.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 47)) (23.2)\n",
      "Requirement already satisfied: platformdirs==4.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 48)) (4.1.0)\n",
      "Requirement already satisfied: proto-plus==1.23.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 49)) (1.23.0)\n",
      "Requirement already satisfied: protobuf==4.25.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 50)) (4.25.1)\n",
      "Requirement already satisfied: pyasn1==0.5.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 51)) (0.5.1)\n",
      "Requirement already satisfied: pyasn1-modules==0.3.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (0.3.0)\n",
      "Requirement already satisfied: pydantic==2.5.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 53)) (2.5.3)\n",
      "Requirement already satisfied: pydantic_core==2.14.6 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (2.14.6)\n",
      "Requirement already satisfied: pylint==3.0.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 55)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 56)) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (1.0.0)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (6.0.1)\n",
      "Requirement already satisfied: requests==2.31.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 59)) (2.31.0)\n",
      "Requirement already satisfied: rsa==4.9 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (4.9)\n",
      "Requirement already satisfied: shapely==2.0.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (2.0.2)\n",
      "Requirement already satisfied: six==1.16.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 62)) (1.16.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 63)) (1.3.0)\n",
      "Requirement already satisfied: SQLAlchemy==2.0.23 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 64)) (2.0.23)\n",
      "Requirement already satisfied: starlette==0.27.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 65)) (0.27.0)\n",
      "Requirement already satisfied: tenacity==8.2.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 66)) (8.2.3)\n",
      "Requirement already satisfied: tomlkit==0.12.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 67)) (0.12.3)\n",
      "Requirement already satisfied: types-protobuf==4.24.0.20240302 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 68)) (4.24.0.20240302)\n",
      "Requirement already satisfied: types-requests==2.31.0.20240218 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 69)) (2.31.0.20240218)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 70)) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions==4.9.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 71)) (4.9.0)\n",
      "Requirement already satisfied: urllib3==2.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 72)) (2.1.0)\n",
      "Requirement already satisfied: uvicorn==0.24.0.post1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 73)) (0.24.0.post1)\n",
      "Requirement already satisfied: yarl==1.9.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 74)) (1.9.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from SQLAlchemy==2.0.23->-r requirements.txt (line 64)) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "! /opt/conda/envs/python3-11-6/bin/python3.11 -m pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff694550-04d4-46e4-b3f1-511a24a3704b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8786bb11-107d-41a8-8440-a9c6e7ae59d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID=\"engaged-domain-403109\"\n",
    "REGION=\"asia-southeast1\"\n",
    "GCS_BUCKET=\"engaged-domain-403109-me-bucket\"\n",
    "ME_INDEX_ID=\"projects/510519063638/locations/asia-southeast1/indexes/4693366538231611392\"\n",
    "ME_ENDPOINT_ID=\"projects/510519063638/locations/asia-southeast1/indexEndpoints/3617586769429528576\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690ec3e-9c0a-4603-89a9-f17c3658bebd",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee987553-4ac8-4ca4-985c-6eb70989002b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue, StringPromptValue\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def debug_fn(x):\n",
    "    \"\"\"This function takes a generic, and prints it before passing it on to the next function.\n",
    "\n",
    "    Think of it as a middleware.\n",
    "\n",
    "    Examples: \n",
    "    answer = {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            # pylint: disable-next=not-callable\n",
    "            \"answer\": final_inputs | ANSWER_PROMPT | debug_fn | self.model,\n",
    "            \"docs\": itemgetter(\"docs\"),\n",
    "        }\n",
    "\n",
    "    standalone_question = {\n",
    "            \"standalone_question\": {\n",
    "                \"question\": lambda x: x[\"question\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            }\n",
    "            | CONDENSE_QUESTION_PROMPT\n",
    "            | debug_fn\n",
    "            | self.model\n",
    "            | StrOutputParser(),\n",
    "        }\n",
    "    \"\"\"\n",
    "    if isinstance(x, (ChatPromptValue, StringPromptValue)):\n",
    "        prompt_val = x.to_string()\n",
    "        pp.pprint({\n",
    "          \"len\": len(prompt_val),\n",
    "          \"prompt_val\": prompt_val,\n",
    "        })\n",
    "    else:\n",
    "        # Prints input as is\n",
    "        pp.pprint(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e732d5-9a4c-48ea-80f7-548a75f16055",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b5710b-13af-4e60-8935-ac05f7071b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load adapter matrix\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/embedding_adapter_training/adapter_matrix.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     best_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomMatchingEngine\u001b[39;00m(VectorStore):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Load adapter matrix\n",
    "with open('data/embedding_adapter_training/adapter_matrix.pickle', 'rb') as f:\n",
    "    best_matrix = pickle.load(f)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomMatchingEngine(VectorStore):\n",
    "    \"\"\"`Google Vertex AI Vector Search` (previously Matching Engine) vector store.\n",
    "\n",
    "    While the embeddings are stored in the Matching Engine, the embedded\n",
    "    documents will be stored in GCS.\n",
    "\n",
    "    An existing Index and corresponding Endpoint are preconditions for\n",
    "    using this module.\n",
    "\n",
    "    See usage in docs/integrations/vectorstores/google_vertex_ai_vector_search.ipynb\n",
    "\n",
    "    Note that this implementation is mostly meant for reading if you are\n",
    "    planning to do a real time implementation. While reading is a real time\n",
    "    operation, updating the index takes close to one hour.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        index: MatchingEngineIndex,\n",
    "        endpoint: MatchingEngineIndexEndpoint,\n",
    "        embedding: Embeddings,\n",
    "        adapted_embedding: Embeddings,\n",
    "        gcs_client: storage.Client,\n",
    "        gcs_bucket_name: str,\n",
    "        credentials: Optional[Credentials] = None,\n",
    "        *,\n",
    "        document_id_key: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Google Vertex AI Vector Search (previously Matching Engine)\n",
    "         implementation of the vector store.\n",
    "\n",
    "        While the embeddings are stored in the Matching Engine, the embedded\n",
    "        documents will be stored in GCS.\n",
    "\n",
    "        An existing Index and corresponding Endpoint are preconditions for\n",
    "        using this module.\n",
    "\n",
    "        See usage in\n",
    "        docs/integrations/vectorstores/google_vertex_ai_vector_search.ipynb.\n",
    "\n",
    "        Note that this implementation is mostly meant for reading if you are\n",
    "        planning to do a real time implementation. While reading is a real time\n",
    "        operation, updating the index takes close to one hour.\n",
    "\n",
    "        Attributes:\n",
    "            project_id: The GCS project id.\n",
    "            index: The created index class. See\n",
    "                ~:func:`MatchingEngine.from_components`.\n",
    "            endpoint: The created endpoint class. See\n",
    "                ~:func:`MatchingEngine.from_components`.\n",
    "            embedding: A :class:`Embeddings` that will be used for\n",
    "                embedding the text sent. If none is sent, then the\n",
    "                multilingual Tensorflow Universal Sentence Encoder will be used.\n",
    "            gcs_client: The GCS client.\n",
    "            gcs_bucket_name: The GCS bucket name.\n",
    "            credentials (Optional): Created GCP credentials.\n",
    "            document_id_key (Optional): Key for storing document ID in document\n",
    "                metadata. If None, document ID will not be returned in document\n",
    "                metadata.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._validate_google_libraries_installation()\n",
    "\n",
    "        self.project_id = project_id\n",
    "        self.index = index\n",
    "        self.endpoint = endpoint\n",
    "        self.embedding = embedding\n",
    "        self.adapted_embedding = adapted_embedding\n",
    "        self.gcs_client = gcs_client\n",
    "        self.credentials = credentials\n",
    "        self.gcs_bucket_name = gcs_bucket_name\n",
    "        self.document_id_key = document_id_key\n",
    "\n",
    "    @property\n",
    "    def embeddings(self) -> Embeddings:\n",
    "        return self.embedding\n",
    "\n",
    "    def _validate_google_libraries_installation(self) -> None:\n",
    "        \"\"\"Validates that Google libraries that are needed are installed.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import aiplatform, storage  # noqa: F401\n",
    "            from google.oauth2 import service_account  # noqa: F401\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"You must run `pip install --upgrade \"\n",
    "                \"google-cloud-aiplatform google-cloud-storage`\"\n",
    "                \"to use the MatchingEngine Vectorstore.\"\n",
    "            )\n",
    "    def similarity_search_with_score(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 4,\n",
    "        filter: Optional[List[Namespace]] = None,\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Return docs most similar to query and their cosine distance from the query.\n",
    "\n",
    "        Args:\n",
    "            query: String query look up documents similar to.\n",
    "            k: Number of Documents to return. Defaults to 4.\n",
    "            filter: Optional. A list of Namespaces for filtering\n",
    "                the matching results.\n",
    "                For example:\n",
    "                [Namespace(\"color\", [\"red\"], []), Namespace(\"shape\", [], [\"squared\"])]\n",
    "                will match datapoints that satisfy \"red color\" but not include\n",
    "                datapoints with \"squared shape\". Please refer to\n",
    "                https://cloud.google.com/vertex-ai/docs/matching-engine/filtering#json\n",
    "                for more detail.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Document, float]]: List of documents most similar to\n",
    "            the query text and cosine distance in float for each.\n",
    "            Lower score represents more similarity.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Embedding query {query}.\")\n",
    "        embedding_query = self.adapted_embedding.embed_query(query)\n",
    "        return self.similarity_search_by_vector_with_score(\n",
    "            embedding_query, k=k, filter=filter\n",
    "        )\n",
    "\n",
    "    def similarity_search_by_vector_with_score(\n",
    "        self,\n",
    "        embedding: List[float],\n",
    "        k: int = 4,\n",
    "        filter: Optional[List[Namespace]] = None,\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Return docs most similar to the embedding and their cosine distance.\n",
    "\n",
    "        Args:\n",
    "            embedding: Embedding to look up documents similar to.\n",
    "            k: Number of Documents to return. Defaults to 4.\n",
    "            filter: Optional. A list of Namespaces for filtering\n",
    "                the matching results.\n",
    "                For example:\n",
    "                [Namespace(\"color\", [\"red\"], []), Namespace(\"shape\", [], [\"squared\"])]\n",
    "                will match datapoints that satisfy \"red color\" but not include\n",
    "                datapoints with \"squared shape\". Please refer to\n",
    "                https://cloud.google.com/vertex-ai/docs/matching-engine/filtering#json\n",
    "                for more detail.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Document, float]]: List of documents most similar to\n",
    "            the query text and cosine distance in float for each.\n",
    "            Lower score represents more similarity.\n",
    "\n",
    "        \"\"\"\n",
    "        filter = filter or []\n",
    "\n",
    "        # If the endpoint is public we use the find_neighbors function.\n",
    "        if hasattr(self.endpoint, \"_public_match_client\") and (\n",
    "            self.endpoint._public_match_client\n",
    "        ):\n",
    "            response = self.endpoint.find_neighbors(\n",
    "                deployed_index_id=self._get_index_id(),\n",
    "                queries=[embedding],\n",
    "                num_neighbors=k,\n",
    "                filter=filter,\n",
    "            )\n",
    "        else:\n",
    "            response = self.endpoint.match(\n",
    "                deployed_index_id=self._get_index_id(),\n",
    "                queries=[embedding],\n",
    "                num_neighbors=k,\n",
    "                filter=filter,\n",
    "            )\n",
    "\n",
    "        # NOTE: Rerun document retrieval if documents collected is less than k \n",
    "        #       (due to current vector store having invalid document for a particular index)\n",
    "        for result in response[0]:\n",
    "            if result.id == '025dfeb0-0468-489c-9008-f0a5ed1d24ad':\n",
    "                if hasattr(self.endpoint, \"_public_match_client\") and (\n",
    "                    self.endpoint._public_match_client\n",
    "                ):\n",
    "                    response = self.endpoint.find_neighbors(\n",
    "                        deployed_index_id=self._get_index_id(),\n",
    "                        queries=[embedding],\n",
    "                        num_neighbors=k+1,\n",
    "                        filter=filter,\n",
    "                    )\n",
    "                else:\n",
    "                    response = self.endpoint.match(\n",
    "                        deployed_index_id=self._get_index_id(),\n",
    "                        queries=[embedding],\n",
    "                        num_neighbors=k+1,\n",
    "                        filter=filter,\n",
    "                    )\n",
    "                break\n",
    "                \n",
    "        logger.debug(f\"Found {len(response)} matches.\")\n",
    "\n",
    "        if len(response) == 0:\n",
    "            return []\n",
    "\n",
    "        docs: List[Tuple[Document, float]] = []\n",
    "        \n",
    "        # I'm only getting the first one because queries receives an array\n",
    "        # and the similarity_search method only receives one query. This\n",
    "        # means that the match method will always return an array with only\n",
    "        # one element.\n",
    "        for result in response[0]:\n",
    "            try:\n",
    "                page_content = self._download_from_gcs(f\"documents/{result.id}\")\n",
    "                # TODO: return all metadata.\n",
    "                metadata = {}\n",
    "                if self.document_id_key is not None:\n",
    "                    metadata[self.document_id_key] = result.id\n",
    "                document = Document(\n",
    "                    page_content=page_content,\n",
    "                    metadata=metadata,\n",
    "                )\n",
    "                docs.append((document, result.distance))\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "\n",
    "        logger.debug(\"Downloaded documents for query.\")\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def _get_index_id(self) -> str:\n",
    "        \"\"\"Gets the correct index id for the endpoint.\n",
    "\n",
    "        Returns:\n",
    "            The index id if found (which should be found) or throws\n",
    "            ValueError otherwise.\n",
    "        \"\"\"\n",
    "        for index in self.endpoint.deployed_indexes:\n",
    "            if index.index == self.index.resource_name:\n",
    "                return index.id\n",
    "\n",
    "        raise ValueError(\n",
    "            f\"No index with id {self.index.resource_name} \"\n",
    "            f\"deployed on endpoint \"\n",
    "            f\"{self.endpoint.display_name}.\"\n",
    "        )\n",
    "\n",
    "    def _download_from_gcs(self, gcs_location: str) -> str:\n",
    "        \"\"\"Downloads from GCS in text format.\n",
    "\n",
    "        Args:\n",
    "            gcs_location: The location where the file is located.\n",
    "\n",
    "        Returns:\n",
    "            The string contents of the file.\n",
    "        \"\"\"\n",
    "        bucket = self.gcs_client.get_bucket(self.gcs_bucket_name)\n",
    "        blob = bucket.blob(gcs_location)\n",
    "        return blob.download_as_string()\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(\n",
    "        cls: Type[\"MatchingEngine\"],\n",
    "        texts: List[str],\n",
    "        embedding: Embeddings,\n",
    "        metadatas: Optional[List[dict]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"MatchingEngine\":\n",
    "        \"\"\"Use from components instead.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"This method is not implemented. Instead, you should initialize the class\"\n",
    "            \" with `MatchingEngine.from_components(...)` and then call \"\n",
    "            \"`add_texts`\"\n",
    "        )\n",
    "    def add_texts(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        metadatas: Optional[List[dict]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
    "\n",
    "        Args:\n",
    "            texts: Iterable of strings to add to the vectorstore.\n",
    "            metadatas: Optional list of metadatas associated with the texts.\n",
    "            kwargs: vectorstore specific parameters.\n",
    "\n",
    "        Returns:\n",
    "            List of ids from adding the texts into the vectorstore.\n",
    "        \"\"\"\n",
    "        texts = list(texts)\n",
    "        if metadatas is not None and len(texts) != len(metadatas):\n",
    "            raise ValueError(\n",
    "                \"texts and metadatas do not have the same length. Received \"\n",
    "                f\"{len(texts)} texts and {len(metadatas)} metadatas.\"\n",
    "            )\n",
    "        logger.debug(\"Embedding documents.\")\n",
    "        embeddings = self.embedding.embed_documents(texts)\n",
    "        jsons = []\n",
    "        ids = []\n",
    "        # Could be improved with async.\n",
    "        for idx, (embedding, text) in enumerate(zip(embeddings, texts)):\n",
    "            id = str(uuid.uuid4())\n",
    "            ids.append(id)\n",
    "            json_: dict = {\"id\": id, \"embedding\": embedding}\n",
    "            if metadatas is not None:\n",
    "                json_[\"metadata\"] = metadatas[idx]\n",
    "            jsons.append(json_)\n",
    "            self._upload_to_gcs(text, f\"documents/{id}\")\n",
    "\n",
    "        logger.debug(f\"Uploaded {len(ids)} documents to GCS.\")\n",
    "\n",
    "        # Creating json lines from the embedded documents.\n",
    "        result_str = \"\\n\".join([json.dumps(x) for x in jsons])\n",
    "\n",
    "        filename_prefix = f\"indexes/{uuid.uuid4()}\"\n",
    "        filename = f\"{filename_prefix}/{time.time()}.json\"\n",
    "        self._upload_to_gcs(result_str, filename)\n",
    "        logger.debug(\n",
    "            f\"Uploaded updated json with embeddings to \"\n",
    "            f\"{self.gcs_bucket_name}/{filename}.\"\n",
    "        )\n",
    "\n",
    "        self.index = self.index.update_embeddings(\n",
    "            contents_delta_uri=f\"gs://{self.gcs_bucket_name}/{filename_prefix}/\"\n",
    "        )\n",
    "\n",
    "        logger.debug(\"Updated index with new configuration.\")\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def similarity_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 4,\n",
    "        filter: Optional[List[Namespace]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Return docs most similar to query.\n",
    "\n",
    "        Args:\n",
    "            query: The string that will be used to search for similar documents.\n",
    "            k: The amount of neighbors that will be retrieved.\n",
    "            filter: Optional. A list of Namespaces for filtering the matching results.\n",
    "                For example:\n",
    "                [Namespace(\"color\", [\"red\"], []), Namespace(\"shape\", [], [\"squared\"])]\n",
    "                will match datapoints that satisfy \"red color\" but not include\n",
    "                datapoints with \"squared shape\". Please refer to\n",
    "                https://cloud.google.com/vertex-ai/docs/matching-engine/filtering#json\n",
    "                 for more detail.\n",
    "\n",
    "        Returns:\n",
    "            A list of k matching documents.\n",
    "        \"\"\"\n",
    "        docs_and_scores = self.similarity_search_with_score(\n",
    "            query, k=k, filter=filter, **kwargs\n",
    "        )\n",
    "\n",
    "        return [doc for doc, _ in docs_and_scores]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_components(\n",
    "        cls: Type[\"MatchingEngine\"],\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        gcs_bucket_name: str,\n",
    "        index_id: str,\n",
    "        endpoint_id: str,\n",
    "        credentials_path: Optional[str] = None,\n",
    "        embedding: Optional[Embeddings] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"MatchingEngine\":\n",
    "        \"\"\"Takes the object creation out of the constructor.\n",
    "\n",
    "        Args:\n",
    "            project_id: The GCP project id.\n",
    "            region: The default location making the API calls. It must have\n",
    "            the same location as the GCS bucket and must be regional.\n",
    "            gcs_bucket_name: The location where the vectors will be stored in\n",
    "            order for the index to be created.\n",
    "            index_id: The id of the created index.\n",
    "            endpoint_id: The id of the created endpoint.\n",
    "            credentials_path: (Optional) The path of the Google credentials on\n",
    "            the local file system.\n",
    "            embedding: The :class:`Embeddings` that will be used for\n",
    "            embedding the texts.\n",
    "            kwargs: Additional keyword arguments to pass to MatchingEngine.__init__().\n",
    "\n",
    "        Returns:\n",
    "            A configured MatchingEngine with the texts added to the index.\n",
    "        \"\"\"\n",
    "        gcs_bucket_name = cls._validate_gcs_bucket(gcs_bucket_name)\n",
    "        credentials = cls._create_credentials_from_file(credentials_path)\n",
    "        index = cls._create_index_by_id(index_id, project_id, region, credentials)\n",
    "        endpoint = cls._create_endpoint_by_id(\n",
    "            endpoint_id, project_id, region, credentials\n",
    "        )\n",
    "\n",
    "        gcs_client = cls._get_gcs_client(credentials, project_id)\n",
    "        cls._init_aiplatform(project_id, region, gcs_bucket_name, credentials)\n",
    "\n",
    "        return cls(\n",
    "            project_id=project_id,\n",
    "            index=index,\n",
    "            endpoint=endpoint,\n",
    "            embedding=embedding or cls._get_default_embeddings(),\n",
    "            gcs_client=gcs_client,\n",
    "            credentials=credentials,\n",
    "            gcs_bucket_name=gcs_bucket_name,\n",
    "            **kwargs,\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def _validate_gcs_bucket(cls, gcs_bucket_name: str) -> str:\n",
    "        \"\"\"Validates the gcs_bucket_name as a bucket name.\n",
    "\n",
    "        Args:\n",
    "              gcs_bucket_name: The received bucket uri.\n",
    "\n",
    "        Returns:\n",
    "              A valid gcs_bucket_name or throws ValueError if full path is\n",
    "              provided.\n",
    "        \"\"\"\n",
    "        gcs_bucket_name = gcs_bucket_name.replace(\"gs://\", \"\")\n",
    "        if \"/\" in gcs_bucket_name:\n",
    "            raise ValueError(\n",
    "                f\"The argument gcs_bucket_name should only be \"\n",
    "                f\"the bucket name. Received {gcs_bucket_name}\"\n",
    "            )\n",
    "        return gcs_bucket_name\n",
    "\n",
    "    @classmethod\n",
    "    def _create_credentials_from_file(\n",
    "        cls, json_credentials_path: Optional[str]\n",
    "    ) -> Optional[Credentials]:\n",
    "        \"\"\"Creates credentials for GCP.\n",
    "\n",
    "        Args:\n",
    "             json_credentials_path: The path on the file system where the\n",
    "             credentials are stored.\n",
    "\n",
    "         Returns:\n",
    "             An optional of Credentials or None, in which case the default\n",
    "             will be used.\n",
    "        \"\"\"\n",
    "\n",
    "        from google.oauth2 import service_account\n",
    "\n",
    "        credentials = None\n",
    "        if json_credentials_path is not None:\n",
    "            credentials = service_account.Credentials.from_service_account_file(\n",
    "                json_credentials_path\n",
    "            )\n",
    "\n",
    "        return credentials\n",
    "\n",
    "    @classmethod\n",
    "    def _create_index_by_id(\n",
    "        cls, index_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
    "    ) -> MatchingEngineIndex:\n",
    "        \"\"\"Creates a MatchingEngineIndex object by id.\n",
    "\n",
    "        Args:\n",
    "            index_id: The created index id.\n",
    "            project_id: The project to retrieve index from.\n",
    "            region: Location to retrieve index from.\n",
    "            credentials: GCS credentials.\n",
    "\n",
    "        Returns:\n",
    "            A configured MatchingEngineIndex.\n",
    "        \"\"\"\n",
    "\n",
    "        from google.cloud import aiplatform\n",
    "\n",
    "        logger.debug(f\"Creating matching engine index with id {index_id}.\")\n",
    "        return aiplatform.MatchingEngineIndex(\n",
    "            index_name=index_id,\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            credentials=credentials,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _create_endpoint_by_id(\n",
    "        cls, endpoint_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
    "    ) -> MatchingEngineIndexEndpoint:\n",
    "        \"\"\"Creates a MatchingEngineIndexEndpoint object by id.\n",
    "\n",
    "        Args:\n",
    "            endpoint_id: The created endpoint id.\n",
    "            project_id: The project to retrieve index from.\n",
    "            region: Location to retrieve index from.\n",
    "            credentials: GCS credentials.\n",
    "\n",
    "        Returns:\n",
    "            A configured MatchingEngineIndexEndpoint.\n",
    "        \"\"\"\n",
    "\n",
    "        from google.cloud import aiplatform\n",
    "\n",
    "        logger.debug(f\"Creating endpoint with id {endpoint_id}.\")\n",
    "        return aiplatform.MatchingEngineIndexEndpoint(\n",
    "            index_endpoint_name=endpoint_id,\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            credentials=credentials,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_gcs_client(\n",
    "        cls, credentials: \"Credentials\", project_id: str\n",
    "    ) -> \"storage.Client\":\n",
    "        \"\"\"Lazily creates a GCS client.\n",
    "\n",
    "        Returns:\n",
    "            A configured GCS client.\n",
    "        \"\"\"\n",
    "\n",
    "        from google.cloud import storage\n",
    "\n",
    "        return storage.Client(\n",
    "            credentials=credentials,\n",
    "            project=project_id,\n",
    "            client_info=get_client_info(module=\"vertex-ai-matching-engine\"),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _init_aiplatform(\n",
    "        cls,\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        gcs_bucket_name: str,\n",
    "        credentials: \"Credentials\",\n",
    "    ) -> None:\n",
    "        \"\"\"Configures the aiplatform library.\n",
    "\n",
    "        Args:\n",
    "            project_id: The GCP project id.\n",
    "            region: The default location making the API calls. It must have\n",
    "            the same location as the GCS bucket and must be regional.\n",
    "            gcs_bucket_name: GCS staging location.\n",
    "            credentials: The GCS Credentials object.\n",
    "        \"\"\"\n",
    "\n",
    "        from google.cloud import aiplatform\n",
    "\n",
    "        logger.debug(\n",
    "            f\"Initializing AI Platform for project {project_id} on \"\n",
    "            f\"{region} and for {gcs_bucket_name}.\"\n",
    "        )\n",
    "        aiplatform.init(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            staging_bucket=gcs_bucket_name,\n",
    "            credentials=credentials,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_default_embeddings(cls) -> \"TensorflowHubEmbeddings\":\n",
    "        \"\"\"This function returns the default embedding.\n",
    "\n",
    "        Returns:\n",
    "            Default TensorflowHubEmbeddings to use.\n",
    "        \"\"\"\n",
    "\n",
    "        from langchain_community.embeddings import TensorflowHubEmbeddings\n",
    "\n",
    "        return TensorflowHubEmbeddings()\n",
    "    \n",
    "\n",
    "class CustomVertexAIEmbeddings_v2(VertexAIEmbeddings, BaseModel):\n",
    "\n",
    "    # Overriding embed_query method\n",
    "    def embed_query(self, text: str):\n",
    "        embeddings = self.client.get_embeddings([text])[0].values\n",
    "        adapted_query_embeddings = np.matmul(best_matrix, np.array(embeddings).T).tolist()\n",
    "        \n",
    "        return adapted_query_embeddings\n",
    "\n",
    "embeddings_v2 = CustomVertexAIEmbeddings_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255231b5-5cbd-4cce-a24b-8c530386d081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain.vectorstores.matching_engine import MatchingEngine\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "\n",
    "def get_vector_search_retriever():\n",
    "    \"\"\"\n",
    "    This method returns a retriever using vector search (ie. Matching Engine)\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = VertexAIEmbeddings(location=REGION, model_name=\"textembedding-gecko@001\")\n",
    "\n",
    "    # me = MatchingEngine.from_components(\n",
    "    #     project_id=PROJECT_ID,\n",
    "    #     region=REGION,\n",
    "    #     gcs_bucket_name=GCS_BUCKET,\n",
    "    #     embedding=embeddings,\n",
    "    #     index_id=ME_INDEX_ID,\n",
    "    #     endpoint_id=ME_ENDPOINT_ID,\n",
    "    # )\n",
    "    me_v2 = CustomMatchingEngine.from_components(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        gcs_bucket_name=GCS_BUCKET,\n",
    "        embedding=embeddings,\n",
    "        adapted_embedding=embeddings_v2,\n",
    "        index_id=ME_INDEX_ID,\n",
    "        endpoint_id=ME_ENDPOINT_ID,\n",
    "    )\n",
    "\n",
    "\n",
    "    NUMBER_OF_RESULTS = 4\n",
    "\n",
    "    # Expose index to the retriever\n",
    "    # https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.matching_engine.MatchingEngine.html?highlight=matchingengine#langchain_community.vectorstores.matching_engine.MatchingEngine.as_retriever\n",
    "    retriever = me_v2.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": NUMBER_OF_RESULTS,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def get_memory_retriever():\n",
    "    \"\"\"\n",
    "    This method returns a vector store retriever that retrieves stored memories\n",
    "    \"\"\"\n",
    "    EMBEDDING_SIZE = 768\n",
    "    index = faiss.IndexFlatL2(EMBEDDING_SIZE)\n",
    "    embedding_fn = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n",
    "\n",
    "    # pylint: disable-next=not-callable\n",
    "    vectorstore_memory = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
    "\n",
    "    retriever = vectorstore_memory.as_retriever(search_kwargs={\"k\": 2})\n",
    "    memory = VectorStoreRetrieverMemory(\n",
    "        retriever=retriever,\n",
    "        return_messages=True,\n",
    "        input_key=\"human\",\n",
    "        output_key=\"ai\"\n",
    "    )\n",
    "\n",
    "    return memory\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3-11-6",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": "python3-11-6 (Local)",
   "language": "python",
   "name": "python3-11-6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
