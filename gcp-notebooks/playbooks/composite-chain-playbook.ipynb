{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f809d4de-d111-4f50-adfe-b85601c15f1f",
   "metadata": {},
   "source": [
    "# Composite Chain\n",
    "\n",
    "This chain takes in the output from conversation retrieval chain and references the eligiblity table to decided whether to return the original answer or a paraphrased response that guides the conversation along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c10794-7dac-40f7-a828-04fac730725e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.9.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: annotated-types==0.6.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: anyio==3.7.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
      "Requirement already satisfied: astroid==3.0.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: attrs==23.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (23.1.0)\n",
      "Requirement already satisfied: cachetools==5.3.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (5.3.2)\n",
      "Requirement already satisfied: certifi==2023.11.17 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (8.1.7)\n",
      "Requirement already satisfied: dataclasses-json==0.6.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.6.3)\n",
      "Requirement already satisfied: dill==0.3.7 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.3.7)\n",
      "Requirement already satisfied: faiss-cpu==1.7.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.7.4)\n",
      "Requirement already satisfied: fastapi==0.105.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.105.0)\n",
      "Requirement already satisfied: frozenlist==1.4.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.4.1)\n",
      "Requirement already satisfied: google-api-core==2.15.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (2.15.0)\n",
      "Requirement already satisfied: google-auth==2.25.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (2.25.2)\n",
      "Collecting google-cloud-aiplatform==1.43.0 (from -r requirements.txt (line 18))\n",
      "  Downloading google_cloud_aiplatform-1.43.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery==3.14.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (3.14.1)\n",
      "Requirement already satisfied: google-cloud-core==2.4.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager==1.11.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (1.11.0)\n",
      "Requirement already satisfied: google-cloud-storage==2.14.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (2.14.0)\n",
      "Requirement already satisfied: google-crc32c==1.5.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (1.5.0)\n",
      "Requirement already satisfied: google-resumable-media==2.7.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (2.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos==1.62.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (1.62.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1==0.13.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (0.13.0)\n",
      "Requirement already satisfied: grpcio==1.60.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (1.60.0)\n",
      "Requirement already satisfied: grpcio-status==1.60.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (1.60.0)\n",
      "Requirement already satisfied: gunicorn==21.2.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (21.2.0)\n",
      "Requirement already satisfied: h11==0.14.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (0.14.0)\n",
      "Requirement already satisfied: idna==3.6 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (3.6)\n",
      "Requirement already satisfied: isort==5.13.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (5.13.2)\n",
      "Requirement already satisfied: jsonpatch==1.33 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (1.33)\n",
      "Requirement already satisfied: jsonpointer==2.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (2.4)\n",
      "Collecting langchain==0.1.10 (from -r requirements.txt (line 35))\n",
      "  Downloading langchain-0.1.10-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-community==0.0.25 (from -r requirements.txt (line 36))\n",
      "  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting langchain-core==0.1.28 (from -r requirements.txt (line 37))\n",
      "  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: langchain-google-vertexai==0.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (0.1.0)\n",
      "Requirement already satisfied: langchain-text-splitters==0.0.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (0.0.1)\n",
      "Collecting langsmith==0.1.10 (from -r requirements.txt (line 40))\n",
      "  Downloading langsmith-0.1.10-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: marshmallow==3.20.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (3.20.1)\n",
      "Requirement already satisfied: mccabe==0.7.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (0.7.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (6.0.4)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (1.0.0)\n",
      "Requirement already satisfied: numpy==1.26.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (1.26.2)\n",
      "Requirement already satisfied: orjson==3.9.15 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (3.9.15)\n",
      "Requirement already satisfied: packaging==23.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 47)) (23.2)\n",
      "Requirement already satisfied: platformdirs==4.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 48)) (4.1.0)\n",
      "Requirement already satisfied: proto-plus==1.23.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 49)) (1.23.0)\n",
      "Requirement already satisfied: protobuf==4.25.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 50)) (4.25.1)\n",
      "Requirement already satisfied: pyasn1==0.5.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 51)) (0.5.1)\n",
      "Requirement already satisfied: pyasn1-modules==0.3.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (0.3.0)\n",
      "Requirement already satisfied: pydantic==2.5.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 53)) (2.5.3)\n",
      "Requirement already satisfied: pydantic_core==2.14.6 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (2.14.6)\n",
      "Requirement already satisfied: pylint==3.0.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 55)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 56)) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (1.0.0)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (6.0.1)\n",
      "Requirement already satisfied: requests==2.31.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 59)) (2.31.0)\n",
      "Requirement already satisfied: rsa==4.9 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (4.9)\n",
      "Requirement already satisfied: shapely==2.0.2 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (2.0.2)\n",
      "Requirement already satisfied: six==1.16.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 62)) (1.16.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 63)) (1.3.0)\n",
      "Requirement already satisfied: SQLAlchemy==2.0.23 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 64)) (2.0.23)\n",
      "Requirement already satisfied: starlette==0.27.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 65)) (0.27.0)\n",
      "Requirement already satisfied: tenacity==8.2.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 66)) (8.2.3)\n",
      "Requirement already satisfied: tomlkit==0.12.3 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 67)) (0.12.3)\n",
      "Requirement already satisfied: types-protobuf==4.24.0.20240302 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 68)) (4.24.0.20240302)\n",
      "Requirement already satisfied: types-requests==2.31.0.20240218 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 69)) (2.31.0.20240218)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 70)) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions==4.9.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 71)) (4.9.0)\n",
      "Requirement already satisfied: urllib3==2.1.0 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 72)) (2.1.0)\n",
      "Requirement already satisfied: uvicorn==0.24.0.post1 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 73)) (0.24.0.post1)\n",
      "Requirement already satisfied: yarl==1.9.4 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from -r requirements.txt (line 74)) (1.9.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/python3-11-6/lib/python3.11/site-packages (from SQLAlchemy==2.0.23->-r requirements.txt (line 64)) (3.0.3)\n",
      "Downloading google_cloud_aiplatform-1.43.0-py2.py3-none-any.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.1.10-py3-none-any.whl (806 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m257.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m261.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.10-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m228.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langsmith, langchain-core, langchain-community, langchain, google-cloud-aiplatform\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.23\n",
      "    Uninstalling langchain-core-0.1.23:\n",
      "      Successfully uninstalled langchain-core-0.1.23\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.20\n",
      "    Uninstalling langchain-community-0.0.20:\n",
      "      Successfully uninstalled langchain-community-0.0.20\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.332\n",
      "    Uninstalling langchain-0.0.332:\n",
      "      Successfully uninstalled langchain-0.0.332\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.35.0\n",
      "    Uninstalling google-cloud-aiplatform-1.35.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.35.0\n",
      "Successfully installed google-cloud-aiplatform-1.43.0 langchain-0.1.10 langchain-community-0.0.25 langchain-core-0.1.28 langsmith-0.1.10\n"
     ]
    }
   ],
   "source": [
    "! /opt/conda/envs/python3-11-6/bin/python3.11 -m pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff694550-04d4-46e4-b3f1-511a24a3704b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8786bb11-107d-41a8-8440-a9c6e7ae59d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID=\"engaged-domain-403109\"\n",
    "REGION=\"asia-southeast1\"\n",
    "GCS_BUCKET=\"engaged-domain-403109-me-bucket\"\n",
    "ME_INDEX_ID=\"projects/510519063638/locations/asia-southeast1/indexes/4693366538231611392\"\n",
    "ME_ENDPOINT_ID=\"projects/510519063638/locations/asia-southeast1/indexEndpoints/3617586769429528576\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690ec3e-9c0a-4603-89a9-f17c3658bebd",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee987553-4ac8-4ca4-985c-6eb70989002b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue, StringPromptValue\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def debug_fn(x):\n",
    "    \"\"\"This function takes a generic, and prints it before passing it on to the next function.\n",
    "\n",
    "    Think of it as a middleware.\n",
    "\n",
    "    Examples: \n",
    "    answer = {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            # pylint: disable-next=not-callable\n",
    "            \"answer\": final_inputs | ANSWER_PROMPT | debug_fn | self.model,\n",
    "            \"docs\": itemgetter(\"docs\"),\n",
    "        }\n",
    "\n",
    "    standalone_question = {\n",
    "            \"standalone_question\": {\n",
    "                \"question\": lambda x: x[\"question\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            }\n",
    "            | CONDENSE_QUESTION_PROMPT\n",
    "            | debug_fn\n",
    "            | self.model\n",
    "            | StrOutputParser(),\n",
    "        }\n",
    "    \"\"\"\n",
    "    if isinstance(x, (ChatPromptValue, StringPromptValue)):\n",
    "        prompt_val = x.to_string()\n",
    "        pp.pprint({\n",
    "          \"len\": len(prompt_val),\n",
    "          \"prompt_val\": prompt_val,\n",
    "        })\n",
    "    else:\n",
    "        # Prints input as is\n",
    "        pp.pprint(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255231b5-5cbd-4cce-a24b-8c530386d081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain.vectorstores.matching_engine import MatchingEngine\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "\n",
    "def get_vector_search_retriever():\n",
    "    \"\"\"\n",
    "    This method returns a retriever using vector search (ie. Matching Engine)\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = VertexAIEmbeddings(location=REGION, model_name=\"textembedding-gecko@001\")\n",
    "\n",
    "    me = MatchingEngine.from_components(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        gcs_bucket_name=GCS_BUCKET,\n",
    "        embedding=embeddings,\n",
    "        index_id=ME_INDEX_ID,\n",
    "        endpoint_id=ME_ENDPOINT_ID,\n",
    "    )\n",
    "\n",
    "    NUMBER_OF_RESULTS = 4\n",
    "\n",
    "    # Expose index to the retriever\n",
    "    # https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.matching_engine.MatchingEngine.html?highlight=matchingengine#langchain_community.vectorstores.matching_engine.MatchingEngine.as_retriever\n",
    "    retriever = me.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": NUMBER_OF_RESULTS,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def get_memory_retriever():\n",
    "    \"\"\"\n",
    "    This method returns a vector store retriever that retrieves stored memories\n",
    "    \"\"\"\n",
    "    EMBEDDING_SIZE = 768\n",
    "    index = faiss.IndexFlatL2(EMBEDDING_SIZE)\n",
    "    embedding_fn = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n",
    "\n",
    "    # pylint: disable-next=not-callable\n",
    "    vectorstore_memory = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
    "\n",
    "    retriever = vectorstore_memory.as_retriever(search_kwargs={\"k\": 2})\n",
    "    memory = VectorStoreRetrieverMemory(\n",
    "        retriever=retriever,\n",
    "        return_messages=True,\n",
    "        input_key=\"human\",\n",
    "        output_key=\"ai\"\n",
    "    )\n",
    "\n",
    "    return memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb350e4-63ae-43e7-9a40-fa01e2e777a4",
   "metadata": {},
   "source": [
    "# Singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50ffb15-8a4a-41fa-8430-b56526525ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "\n",
    "class SingletonMeta(type):\n",
    "    \"\"\"\n",
    "    This is a thread-safe implementation of Singleton.\n",
    "    \"\"\"\n",
    "\n",
    "    _instances = {}\n",
    "\n",
    "    _lock: Lock = Lock()\n",
    "    \"\"\"\n",
    "    We now have a lock object that will be used to synchronize threads during\n",
    "    first access to the Singleton.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Possible changes to the value of the `__init__` argument do not affect\n",
    "        the returned instance.\n",
    "        \"\"\"\n",
    "        # Now, imagine that the program has just been launched. Since there's no\n",
    "        # Singleton instance yet, multiple threads can simultaneously pass the\n",
    "        # previous conditional and reach this point almost at the same time. The\n",
    "        # first of them will acquire lock and will proceed further, while the\n",
    "        # rest will wait here.\n",
    "        with cls._lock:\n",
    "            # The first thread to acquire the lock, reaches this conditional,\n",
    "            # goes inside and creates the Singleton instance. Once it leaves the\n",
    "            # lock block, a thread that might have been waiting for the lock\n",
    "            # release may then enter this section. But since the Singleton field\n",
    "            # is already initialized, the thread won't create a new object.\n",
    "            if cls not in cls._instances:\n",
    "                instance = super().__call__(*args, **kwargs)\n",
    "                cls._instances[cls] = instance\n",
    "        return cls._instances[cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cdf87-3656-4fd9-9c3a-236df00229c8",
   "metadata": {},
   "source": [
    "# Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1bcf23-e35e-4a06-bd4c-4f29a2dac21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SYSTEM_TEMPLATE = \"\"\"You are a helpful assistant from the Singapore government to caregivers or persons with disabilities, including the elderly. You are able to suggest relevant grants and schemes based on their unique circumstances and type of support they are looking for. You are most familiar with financial grants and schemes, but should still be able to answer generic questions related to caregiving or other support for persons with disabilities. \n",
    "# You should first understand what type of support they are looking for. Only then, you should proceed to ask them for more information in order to provide more relevant recommendations. If possible, try to anticipate what they need and assess relevant information based on the context they provide. Determine the subject of their responses based on context of the most recent messages they have sent.\n",
    "# As part of your assessment to provide recommendations, you should ideally consider the beneficiary's age, impairment, activities of daily living that they need assistance with, and the average income per capita in their household. With regard to the activites of daily living, there are six pre-defined categories you should look out for: eating, dressing, toileting, bathing, walking or moving around, transferring from bed to chair and vice versa. \n",
    "\n",
    "# If the user is unwilling to share any information, do not force them to disclose this information, but don't give up. Move on to ask for other details instead. \n",
    "# The more details you know, the more relevant suggestions you can give by narrowing them down based on the eligibility criteria. The less details you know, the more generic suggestions you can give. Even if the user provides no details, try to give them at least the most generic suggestions, even if it means giving them some examples of all available solutions.\n",
    "\n",
    "# English might not be your user's first language. Always ensure that your responses are concise, easy to read and understand.Ask them questions one at a time as follow-up instead of overwhelming them with multiple questions at once. \n",
    "# Your responses should always be empathetic but not sympathetic and respectful to preserve the dignity of the caregiver or persons with disabilities. Always revise your response to replace or explain technical jargons, and match the complexity of language to the human's inputs, without being condescending or using derogatory terms.\"\"\"\n",
    "SYSTEM_TEMPLATE = \"\"\"Imagine you are an assistant from the Singapore government specialising in suggesting relevant financial grants and schemes for caregivers or persons with disabilities including the elderly based on their unique circumstances. \n",
    "Your task is to guide them to understand and decide what they can do next by providing concise answers to their questions, followed by further details on complex topics and relevant suggestions on grants and schemes if necessary. \n",
    "You should focus on providing empathetic and informative responses that prioritise user understanding and actionable recommendations.\"\"\"\n",
    "\n",
    "# TODO: \n",
    "# - I noticed that the topic setting functionality in steps 7-8 is not good when the question is not explicitly about grants.\n",
    "# - For eg, question is \"What is SPED?\", the topic set is \"Caregiving\" when it should be more verbose\n",
    "# - It might help if we be more verbose in Step 8 in describing \"If no....\"\n",
    "# STANDALONE_TEMPLATE = \"\"\"Imagine you are assisting someone who specialises in helping caregivers or persons with disabilities. Your task is to refine a given user question by incorporating relevant context from a given conversation history. Respond with an enhanced standalone question that reflects a deeper understanding of the user's needs. Follow these steps in your response: \n",
    "# 1. Begin by analysing the given user question related to caregivers or persons with disabilities. Identify key themes and keywords without making any assumptions.\n",
    "# 2. Consider the provided conversation history to understand the context of the ongoing discussion and any relevant topics or details. Think critically and identify explain your judgement on how relevant the given pieces of the conversation history is to the user's question.\n",
    "# 3. Integrate only pieces from the conversation history that you have evaluated as relevant as context into the user question without making any assumptions or referring to your own knowledge. If there is no relevant context identified from the conversation history, do not alter the given user question at all, and you should return the same given user question as your refined question. \n",
    "# 5. Ensure that the refined question is clear, coherent and reflects a deeper understanding of the user's situation on its own, without a need refer to any of the given conversation history. \n",
    "# 6. Make an overall judgement on how well the refined standalone question incorporates pertinent details from the given conversation history. \n",
    "# 7. Evaluate if the refined standalone question requires a response with information about specific grants that are either explicitly mentioned in the question, or you identify as relevant examples. \n",
    "# 8. If no, identify the main subject of each topic and set it as a topic. If yes, identify the name of the grant and set it as the topic.\n",
    "# The previous conversation is: \n",
    "# {chat_history}\n",
    "# \n",
    "# \n",
    "# Follow Up Input: {question}\n",
    "# {format_instructions}\n",
    "# \"\"\"\n",
    "STANDALONE_TEMPLATE = \"\"\"Your task is to refine a given user question by weaving in relevant details from a provided chat history. Craft a rephrased question that can be understood independently, capturing the essence of the original query while incorporating contextual nuances. Follow the steps below while ensuring that each transition in thought maintains coherence and clarity, resulting in a refined, standalone question reflective of the conversation's progression: \n",
    "\n",
    "1. Begin by analysing the given user question thoroughly. This entails identifying the main topic, intent, and any specific details or nuances in the query. \n",
    "2. Analyse the provided chat history and make a judgement if there is relevant information that can enhance the understanding of the given user question. Look for context, background details or previous discussions related to the main topic at hand. If there is no relevant judgement, skip the remaining steps and return the given user question as your response without making any changes. \n",
    "{chat_history}\n",
    "3. Extract and integrate the relevant context into the rephrased question without making any assumptions outside of the given chat history. Transition smoothly from the user's original inquiry to incorporating details from the chat history, ensuring coherence and logical progression. \n",
    "4. Preserve the essence of the user's original question throughout the rephrasing process. Although incorporating additional context, ensure that the core topic and intent remain unchanged. \n",
    "5. Craft the rephrased question in a manner that is standalone, clear and comprehensible. Ensure that a third party can gain a deep understanding of the inquiry in the context of the user's situation without needed to refer back to previous interactions. \n",
    "6. Make a judgement if the rephrased question pertains to a specific grant or scheme, or if it covers a broader topic. \n",
    "7. If the rephrased question is about a specific grant or scheme, dynamically assign the name of the grant or scheme as the main topic. If the question is broader in scope, designate the topic you have identified as the main topic.  \n",
    "8. Make an overall judgement on how well the rephrased question can help a third party understand the inquiry in the context of the user's situation without needing to refer back to previous interactions. \n",
    "\n",
    "Follow Up Input: {question}\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Original \n",
    "# ANSWER_TEMPLATE = \"\"\"Try to answer the question based on the following context:\n",
    "# {context}\n",
    "\n",
    "# The examples are:\n",
    "# {examples}\n",
    "\n",
    "# These examples are only teaching you how to navigate a conversation around a specific topic. You should not replace the current question topic with the example topic.\n",
    "\n",
    "# Be precise and concise with your answer. Do not include half-finished sentences.\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# COT implementation\n",
    "# ANSWER_TEMPLATE = \"\"\"Try to answer the question based on the following context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Topic: {topic}\n",
    "\n",
    "# Examples: {examples}\n",
    "\n",
    "# Follow these steps in your response:\n",
    "# 1. Understanding the intent of the question.\n",
    "# 2. Use the examples a reference in helping you understanding the nature of the input question.\n",
    "# 3. These examples are only teaching you how to navigate a conversation around a specific topic. You should not replace the topic with the example topic.\n",
    "# 4. In addition, reference the topic when crafting your answer.\n",
    "# 3. Be precise and concise with your answer. Do not include half-finished sentences.\n",
    "\n",
    "# {format_instructions}\n",
    "# \"\"\"\n",
    "\n",
    "ANSWER_TEMPLATE = \"\"\"Follow these steps in your response to the given user question:\n",
    "1. Begin by analysing the given user message in the context of financial grants or schemes for caregivers, persons with disabilities and/or the elderly. Identify the main topic, intent and inquiry by the user. \n",
    "2. Generate a clear, comprehensible and concise response without making any assumptions. If the user is asking for examples, include a list of least 2 and up to 3 examples in your response without too much explanation. Do not explicitly reveal or make known your intention of giving them a summary. \n",
    "3. Ensure that your response is <800 characters. \n",
    "4. Ensure that language used should always be respectful, empathetic, not condescending and not derogatory. Always match the complexity of language used in the given user question. \n",
    "5. Ensure that throughout the answering process, your response directly addresses the core topic and intent of the user question based on the given context: \n",
    "{context}\n",
    "6. Make an overall judgement how well your final response addresses the user's question. \n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 1. Evaluate if the question requires a response with information about specific grants that are either explicitly mentioned in the question, or you identify as relevant examples. \n",
    "# 2. If no, skip to point 3. If yes, craft a response for each relevant grant you have identified by following these steps: \n",
    "\n",
    "# TODO: \n",
    "# I noticed that 2c. is replaced by the name of the grant instead of \"About the grant\". It might be because of the introduction of {topic}\n",
    "# in the prompt. \n",
    "# I believe you can remove the topic if you need to. The chain should be able to hanldle that.\n",
    "# INFO_TEMPLATE = \"\"\"Try to provide a list of summarized points based on the following context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Follow these steps in your response:\n",
    "# 1. Evaluate if the topic is a grant and/or the question requires a response with information about specific grants that are either explicitly mentioned in the question, or you identify as relevant examples. \n",
    "# 2. If no, skip to point 3. If yes, craft a response for each relevant grant you have identified by following these steps: \n",
    "# 2a. [Grants] In this case, the topic should be the name of the grant.\n",
    "# 2b. [Grants] For each of the following sub-topics, first set these sub-topics from 2c - 2f as the title. Then, consider the context and craft a description about it.\n",
    "# 2c. [Grants] About the grant\n",
    "# 2d. [Grants] Eligibility\n",
    "# 2e. [Grants] Expected benefits\n",
    "# 2f. [Grants] Application process\n",
    "\n",
    "# 3. Evaluate if the question requires a response with information about a general topics. Craft a response for each relevant topic you have identified by following these steps: \n",
    "# 3a. Identify the main subject of each topic and set it as a topic. \n",
    "# 3b. Consider the context, and craft a description about it. Each description should be self-contained, and should not be mentioned in another description point. Where relevant, include examples in this description\n",
    "\n",
    "# {format_instructions}\n",
    "# \"\"\"\n",
    "\n",
    "INFO_TEMPLATE = \"\"\"Follow these steps to generate a response to the given topic and user question:\n",
    "\n",
    "Topic: {topic}\n",
    "Question: {question} \n",
    "\n",
    "1. Begin by understanding if the given topic is about a specific grant or scheme, or if it covers a broader topic.  \n",
    "2. Analyse the following context to extract relevant information to the given question: \n",
    "{context}\n",
    "If the given topic is about a specific grant or scheme, this should include details about specific grants, eligibility criteria, expected benefits and application processes. \n",
    "If the given topic or question is broader in scope, skip to Step 5.\n",
    "3. Ensure the inclusion of fixed sections with these specific titles in your response: About the grant, Eligibility, Expected benefits, Application process. These sections provide comprehensive information extracted from the context about the grant or scheme to help user's understanding. \n",
    "4. Assess if there are remaining relevant details extracted from the context in Step 2 other than those already covered in Step 3. If no, skip to Step 7.\n",
    "5. Assess if this extracted information from the context contains complex topics that the user may need more details or explanation for better understanding. If yes, assess if it can be broken down into mutually-exclusive sub-topics for clarity and ease of understanding. Ensure that each sub-topic is of sufficient complexity to warrant its own section. If there is no extracted information, or if the information if simple enough not to require additional explanation, skip all remaining steps and return nothing. \n",
    "Choose a maximum of the top 5 most relevant sub-topics as a part of your response.\n",
    "6. If such mutually-exclusive sub-topics are identified, partition the response accordingly, with each sub-topic serving as a distinct title. Ensure seamless integration of relevant information from the given context into each sub-topic. \n",
    "7. If relevant, include a section titled 'Examples'. Provide a minimum of two relevant examples to illustrate key points or concepts, enhancing user's comprehension. \n",
    "8. Throughout the response crafting process, prioritise clarity and coherence. Ensure that the response is structured logically, facilitating easy navigation and understanding for the user. Details in each sub-topic should be self-contained and not mentioned in another sub-topic within the same response. \n",
    "Ensure that language used should always be respectful, empathetic, not condescending and not derogatory. Always match the complexity of language used in the given user question. \n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e29d85-bc77-4f2d-a224-02ec1f3922f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Should be the examples to guide the crafting of standalone quesitons\n",
    "FEWSHOT_ANSWER_EXAMPLES = [\n",
    "    {\n",
    "        \"human\": \"My grandma just had an accident and I don't know what to do.\",\n",
    "        \"ai\": \"I'm really sorry to hear about your grandma's accident. I'm here to help. Could you please provide more details about the situation? What happened, and what kind of assistance or information are you looking for? I'll do my best to support you.\"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"What kind of help can I get when my parents are getting old?\",\n",
    "        \"ai\": \"As your parents age, different types of help are available, like support with daily tasks, healthcare, or financial assistance. To give better suggestions, could you share what your parents need and the type of help you're looking for?\"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"What can I do to help my grandfather?\",\n",
    "        \"ai\": \"I'm glad to help you find support for your grandfather. To give you more relevant suggestions, could you please share what kind of support you're looking for? Are you looking for financial assistance, help with daily tasks, or something else?\"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"What kind of support is available to caregivers?\",\n",
    "        \"ai\": \"Caregivers can get different kinds of help. This includes support with daily tasks, finances, and resources for information. To offer more specific suggestions, could you tell me more about your caregiving situation and the type of support you need?\"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"What are ADLs?\",\n",
    "        \"ai\": \"\"\"\"ADLs, or Activities of Daily Living, are everyday tasks we usually do without help. There are 6 ADLs in Singapore:\n",
    "1. Eating\n",
    "2. Bathing\n",
    "3. Dressing\n",
    "4. Using the toilet\n",
    "5. Moving from a bed to chair or chair to bed\n",
    "6. Walking or moving around\n",
    "It's good to know about ADLs because they can affect your eligibility for certain support grants and schemes. If you have more questions or need examples, feel free to ask!\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"Maybe money?\",\n",
    "        \"ai\": \"That's a good start. To provide more relevant recommendations, could you share more details about your situation? For example, what is the average income per capita in the household?\"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"I don't want to share financial details\",\n",
    "        \"ai\": \"That's okay. You don't have to share that information if you're not comfortable. There are still other ways I could help. Could you tell me more about the situation or challenges your grandparents are facing? \"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"They are on a wheelchair\",\n",
    "        \"ai\": \"\"\"\"I noticed that your grandma uses a wheelchair, which may indicate she might need help with daily activities like:\n",
    "- Bathing\n",
    "- Using the toilet\n",
    "- Moving from bed to chair or chair to bed\n",
    "- Walking or moving around\n",
    "Is this information correct? If you can think of any other activities or have any questions, let me know!\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# No longer in use\n",
    "FEWSHOT_STANDALONE_QUESTION_EXAMPLES = [\n",
    "    {\n",
    "        \"human\": \"What is ADL?\",\n",
    "        \"chat_history\": \"\",\n",
    "        \"ai\": \"What is ADL?\"\n",
    "    },\n",
    "    {\n",
    "        \"human\": \"Name me a few\",\n",
    "        \"chat_history\": \"human:  What is SPED?\\nai: ' SPED stands for Special Education. SPED schools cater to children and youths with special needs who require more intensive and specialised assistance.'\",\n",
    "        \"ai\": \"Name me a few SPED schools in Singapore\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2a169-913f-49fe-a022-d0b9c70c4e07",
   "metadata": {},
   "source": [
    "# CONVERSATIONAL RETRIEVAL CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261a3684-d9b6-4813-9d09-4c445ae31ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, FewShotChatMessagePromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableSequence\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    \"\"\"This method formats documents into a string\"\"\"\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "def get_fewshot_example_selector(examples, k=2):\n",
    "    \"\"\"\n",
    "    This method returns an example selector that select from a series of \n",
    "    examples to dynamically place in-context information into your prompt.\n",
    "    \"\"\"\n",
    "    embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n",
    "    return SemanticSimilarityExampleSelector.from_examples(\n",
    "        examples,\n",
    "        embeddings,\n",
    "        FAISS,\n",
    "        k\n",
    "    )\n",
    "\n",
    "def handle_malformed_json(x):\n",
    "    \"\"\"\n",
    "    Function to handle occasional malformed LLM JSON output\n",
    "\n",
    "    Backticks will be placed at the back of json string, breaking JsonOutputParser\n",
    "    output='{...some json data}\\n```'\n",
    "\n",
    "    The fix is to remove the backticks if it happens.\n",
    "    \"\"\"\n",
    "    if \"\\n```\" in x.content:\n",
    "        x.content = x.content.replace(\"\\n```\", \"\\n\")\n",
    "    return x\n",
    "\n",
    "class StandaloneQuestionOutput(BaseModel):\n",
    "    \"\"\"Typings for standalone question output item\"\"\"\n",
    "    topic: str = Field(\n",
    "        description=\"This the main topic of the refined standalone question.\")\n",
    "    standalone_question: str = Field(\n",
    "        description=\"This is the refined standalone question.\")\n",
    "\n",
    "class InfoItem(BaseModel):\n",
    "    \"\"\"Typings for description item\"\"\"\n",
    "    content: str = Field(\n",
    "        description=\"This is the content string for each InfoItem.\")\n",
    "    title: str = Field(\n",
    "        description=\"This is the title associated with each InfoItem. It summarizes the corresponding content string.\")\n",
    "\n",
    "\n",
    "class InfoOutput(BaseModel):\n",
    "    \"\"\"Typings for descriptions chain output\"\"\"\n",
    "    details: List[InfoItem] = Field(\n",
    "        description=\"This is the list of InfoItems.\")\n",
    "    # explanation: str = Field(\n",
    "    #     description=\"This is the explanation of your thought process in crafting the entire output. Be as thorough and detailed as you can be.\")\n",
    "\n",
    "\n",
    "class ConversationalRetrievalChain():\n",
    "    \"\"\"\n",
    "    This class creates a chain that attempts to FIRST answer user question on the dataset before falling back on its own knowledge.\n",
    "\n",
    "    final_chain = loaded_memory | standalone_question | retrieved_documents | answer / descriptions | updateMemory    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"This method instantiates an instance of ConversationalRetrievalChain\"\"\"\n",
    "        # https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models\n",
    "        # pylint: disable-next=not-callable\n",
    "        self.model = ChatVertexAI(\n",
    "            model_name=\"chat-bison-32k\", temperature=0, max_output_tokens=8192)\n",
    "        self.memory = get_memory_retriever()\n",
    "        self.retriever = get_vector_search_retriever()\n",
    "        self.chain = self.get_chain()\n",
    "\n",
    "    def get_chain(self) -> RunnableSequence:\n",
    "        \"\"\"This method instantiates the chain\"\"\"\n",
    "        loaded_memory = RunnableParallel({\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: self.memory.load_memory_variables({\"human\": x[\"question\"]})[\"history\"]\n",
    "        })\n",
    "\n",
    "        retrieved_documents = RunnablePassthrough.assign(\n",
    "            docs=itemgetter(\"standalone_question\") | self.retriever\n",
    "        )\n",
    "\n",
    "        # get chains\n",
    "        standalone_question_chain = self.get_standalone_question_chain()\n",
    "        answer_chain = self.get_answer_chain()\n",
    "        info_chain = self.get_info_chain()\n",
    "\n",
    "        update_memory = RunnablePassthrough.assign(\n",
    "            _=lambda x: self.save_to_memory(x[\"question\"], x[\"answer\"]),\n",
    "        )\n",
    "\n",
    "        final_chain = (\n",
    "            loaded_memory\n",
    "            | standalone_question_chain\n",
    "            | retrieved_documents\n",
    "            | RunnableParallel({\n",
    "                \"question\": lambda x: x[\"standalone_question\"],\n",
    "                \"topic\": lambda x: x[\"topic\"],\n",
    "                \"answer\": answer_chain,\n",
    "                \"information\": info_chain\n",
    "            })\n",
    "            | update_memory\n",
    "        )\n",
    "\n",
    "        return final_chain\n",
    "\n",
    "    def save_to_memory(self, question: str, answer: str) -> None:\n",
    "        \"\"\"This method saves chat history to memory\"\"\"\n",
    "        self.memory.save_context({\"human\": question}, {\"ai\": answer})\n",
    "\n",
    "    def get_standalone_question_chain(self) -> RunnableSequence:\n",
    "        \"\"\"This method returns the standalone question chain\"\"\"\n",
    "        standalone_question_parser = JsonOutputParser(\n",
    "            pydantic_object=StandaloneQuestionOutput)\n",
    "        CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\n",
    "            STANDALONE_TEMPLATE, partial_variables={\"format_instructions\": standalone_question_parser.get_format_instructions()})\n",
    "\n",
    "        standalone_question_chain = (RunnablePassthrough()\n",
    "                                     | CONDENSE_QUESTION_PROMPT\n",
    "                                     | self.model\n",
    "                                     | standalone_question_parser\n",
    "                                     )\n",
    "\n",
    "        return standalone_question_chain\n",
    "\n",
    "    def get_answer_chain(self) -> RunnableSequence:\n",
    "        \"\"\"This method returns the answer chain\"\"\"\n",
    "        ANSWER_PROMPT = ChatPromptTemplate(messages=[\n",
    "            SystemMessagePromptTemplate.from_template(SYSTEM_TEMPLATE),\n",
    "            HumanMessagePromptTemplate.from_template(ANSWER_TEMPLATE)\n",
    "        ])\n",
    "\n",
    "        FEWSHOT_ANSWER_EXAMPLE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"{human}\"), (\"ai\", \"{ai}\")\n",
    "        ])\n",
    "\n",
    "        FEWSHOT_ANSWER_PROMPT = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=FEWSHOT_ANSWER_EXAMPLE_PROMPT,\n",
    "            example_selector=get_fewshot_example_selector(\n",
    "                FEWSHOT_ANSWER_EXAMPLES, k=2)\n",
    "        )\n",
    "\n",
    "        final_inputs = {\n",
    "            \"context\": lambda x: combine_documents(x[\"docs\"]),\n",
    "            \"topic\": itemgetter('topic'),\n",
    "            \"question\": itemgetter(\"standalone_question\"),\n",
    "            \"examples\": lambda x: FEWSHOT_ANSWER_PROMPT.format(human=x[\"standalone_question\"]),\n",
    "        }\n",
    "\n",
    "        answer_chain = final_inputs | ANSWER_PROMPT | self.model | StrOutputParser()\n",
    "\n",
    "        return answer_chain\n",
    "\n",
    "    def get_info_chain(self) -> RunnableSequence:\n",
    "        \"\"\"This method returns the information chain\"\"\"\n",
    "\n",
    "        info_parser = JsonOutputParser(pydantic_object=InfoOutput)\n",
    "        INFO_PROMPT = PromptTemplate.from_template(INFO_TEMPLATE, partial_variables={\n",
    "            \"format_instructions\": info_parser.get_format_instructions()\n",
    "        })\n",
    "\n",
    "        info_chain = (\n",
    "            {\n",
    "                \"context\": lambda x: combine_documents(x[\"docs\"]),\n",
    "                \"topic\": itemgetter(\"topic\"),\n",
    "                \"question\": itemgetter(\"standalone_question\")\n",
    "            }\n",
    "            | INFO_PROMPT\n",
    "            | ChatVertexAI(model_name=\"chat-bison-32k\", temperature=0, max_output_tokens=8192)\n",
    "            | handle_malformed_json\n",
    "            | info_parser\n",
    "        )\n",
    "\n",
    "        return info_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f56ec8-0c96-4ce4-af45-264edaf8ccca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211c869e-a3a5-4d6e-8d35-c91a2e2da8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CRITERIA_TEMPLATE = \"\"\"\n",
    "Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response:\n",
    "\n",
    "Grading Rubric: {criteria}\n",
    "\n",
    "DATA:\n",
    "---------\n",
    "Question: {question}\n",
    "---------\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "CLASSIFIER_TEMPLATE = \"\"\"\n",
    "Given the user question below, classify it as either being about one of the criteria's in this list\n",
    "{criteria_names}\n",
    "\n",
    "Do not respond with more than one word. \n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3863335e-3259-4114-a125-9a17e1f2d024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Dict\n",
    "\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch, RunnableSequence, RunnableParallel\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class CriteriaEnum(str, Enum):\n",
    "    # \"\"\"The types of the Criteria.\"\"\"\n",
    "    # LOW_INCOME = \"Is the user or the person in question considered low income?\"\n",
    "    # LOW_FINANCIAL_AND_OR_LEGAL_LITERACY = \"Does the person in question demonstrate a limited understanding of fundamental financial and/or legal concepts?\"\n",
    "    # FAMILY_OF_PWD = \"Is the person in question a family of a person with disability?\"\n",
    "    # ACCIDENT = \"Was the user or person in question recently in an accident?\"\n",
    "    \"\"\"The types of the Criteria.\"\"\"\n",
    "    CITIZENSHIP = \"Is the recipient Singaporean or a Singapore Permanent Resident?\"\n",
    "    ADL = \"Does the care recipient need help with 3 or more of 6 Activities of Daily Living, which are bathing, feeding, dressing, moving from bed to chair or chair to bed, using the toilet, walking or moving around?\"\n",
    "    RESIDENTIAL_STATUS = \"Is the recipient currently in a residential long-term institution like a nursing home?\"\n",
    "    RESIDENTIAL_ADDRESS = \"Is the address listed on the care recipient's NRIC a private property?\"\n",
    "    AVG_HOUSEHOLD_INCOME = \"Is the average income per person in the household $2,800 and less?\"\n",
    "    PROPERTY_OWNERSHIP = \"Does the user or their household member own more than 1 property?\"\n",
    "\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"\n",
    "    Typings for Evaluator output\n",
    "    \"\"\"\n",
    "    criteria: str = Field(description=\"criteria\")\n",
    "    reasoning: str = Field(description=\"detailed explanation\")\n",
    "    value: str = Field(description=\"Y/N\")\n",
    "\n",
    "\n",
    "class EvaluationChain():\n",
    "    \"\"\"This class evaluates the user's input\"\"\"\n",
    "\n",
    "    def __init__(self, eligibility_dict: Dict[str, bool]) -> None:\n",
    "        \"\"\"This method instantiates an instance of EvaluationChain\"\"\"\n",
    "        # pylint: disable-next=not-callable\n",
    "        self.model = VertexAI(\n",
    "            model=\"chat-bison-32k\", temperature=0, verbose=True, max_tokens=32768)\n",
    "        self.eligibility_dict = eligibility_dict\n",
    "        self.chain = self.get_chain()\n",
    "\n",
    "    def get_chain(self) -> RunnableSequence:\n",
    "        \"\"\"\n",
    "        This method returns the evaluation chain.\n",
    "\n",
    "        ..evaluation_chain\n",
    "        classifier_chain | RunnableBranch\n",
    "\n",
    "        The classifier_chain classifies the input as one of the types in CriteriaEnum\n",
    "        and passes it to RunnableBranch. The matched runnable is then invoked based\n",
    "        on the value from the classifier_chain.\n",
    "\n",
    "        ..RunnableBranch consists of\n",
    "            1. Default Runnable that returns None if no branches are matched\n",
    "            2. Custom runnables loaded with each criteria in CriteriaEnum\n",
    "        \"\"\"\n",
    "        evaluators = self.get_evaluators()\n",
    "        classifier_chain = self.get_classifier_chain()\n",
    "\n",
    "        def update_and_invoke(evaluator: RunnableSequence, question: str, criteria: str) -> Dict[str, str]:\n",
    "            \"\"\"\n",
    "            This helper function does:\n",
    "            1. Invokes evaluation chain\n",
    "            2. Updates eligibility criteria dictionary based on the outcome in 1\n",
    "            \"\"\"\n",
    "            res = evaluator.invoke(question)\n",
    "\n",
    "            if res[\"value\"] == \"Y\" and self.eligibility_dict[criteria] in [False, None]:\n",
    "                self.eligibility_dict[criteria] = True\n",
    "            return res\n",
    "\n",
    "        branch = RunnableBranch(\n",
    "            *[\n",
    "                (\n",
    "                    lambda x, k=k: k in x[\"criteria\"].strip() and self.eligibility_dict[k] in [\n",
    "                        False, None],\n",
    "                    lambda x, k=k, v=v: update_and_invoke(v, x[\"question\"], k)\n",
    "                ) for k, v in evaluators.items()\n",
    "            ],\n",
    "            lambda x: None\n",
    "        )\n",
    "        eval_chain = (\n",
    "            RunnableParallel({\n",
    "                \"criteria\": classifier_chain,\n",
    "                \"question\": lambda x: x[\"question\"]\n",
    "            })\n",
    "            | branch\n",
    "        )\n",
    "\n",
    "        return eval_chain\n",
    "\n",
    "    def get_classifier_chain(self) -> RunnableSequence:\n",
    "        \"\"\"\n",
    "        This method returns a classifier chain that classifies the question as one\n",
    "        of the criteria defined in CriteriaEnum.\n",
    "\n",
    "        If no criteria is matched, None is returned.\n",
    "        \"\"\"\n",
    "        criteria_names = [criteria.name for criteria in CriteriaEnum]\n",
    "        CLASSIFIER_PROMPT = PromptTemplate.from_template(CLASSIFIER_TEMPLATE)\n",
    "        classifier_chain = (\n",
    "            {\n",
    "                \"question\": RunnablePassthrough() | itemgetter(\"question\"),\n",
    "                \"criteria_names\": lambda x: criteria_names\n",
    "            }\n",
    "            | CLASSIFIER_PROMPT\n",
    "            # pylint: disable-next=not-callable\n",
    "            | VertexAI(verbose=True)\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return classifier_chain\n",
    "\n",
    "    def get_evaluators(self) -> Dict[str, RunnableSequence]:\n",
    "        \"\"\"\n",
    "        This method returns a dictionary of CRITERIA_NAME : CRITERIA_CHAIN\n",
    "        \"\"\"\n",
    "        parser = JsonOutputParser(pydantic_object=Answer)\n",
    "        CRITERIA_PROMPT = PromptTemplate.from_template(\n",
    "            CRITERIA_TEMPLATE, partial_variables={\n",
    "                \"format_instructions\": parser.get_format_instructions()}\n",
    "        )\n",
    "        return {\n",
    "            criteria.name: (\n",
    "                {\n",
    "                    \"question\": RunnablePassthrough(),\n",
    "                    \"criteria\": lambda x, c=criteria: {c.name: c.value}\n",
    "                }\n",
    "                | CRITERIA_PROMPT\n",
    "                # pylint: disable-next=not-callable\n",
    "                | VertexAI(verbose=True)\n",
    "                | parser\n",
    "            ) for criteria in CriteriaEnum\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae74544-44f7-4ad5-b4b1-a880743aef3d",
   "metadata": {},
   "source": [
    "# Composite Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "896cb92c-c952-4796-96b3-8b5bf418fbe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAPHRASE_AND_SUGGEST_TEMPLATE = \"\"\"Follow these steps to paraphrase a given original answer to a given user question: \n",
    "\n",
    "Question: {question}\n",
    "Original answer: {original_answer}\n",
    "\n",
    "\n",
    "1. Begin by analysing the given user question to understand its core topic and intent. Determine if there is an opportunity, explicit or based on your prediction, to assess eligibility for grants or schemes based on original answer addresses it to guide the subsequent steps base on the given eligibility table. \n",
    "Question: {question}\n",
    "Eligibility table: {eligibility_table}\n",
    "2. If there are no more ‘None’ values in the eligibility table, do not rephrase the question and skip to Step 6. \n",
    "3. If there's a relevant opportunity, proceed to Step 3 to paraphrase the given original answer by subtly incorporating consideration of criteria with 'None' values in the given eligibility table. If there is no such relevant opportunity, subtly introduce the topic of eligibility assessment where relevant while ensuring that the paraphrased response maintains the primary focus of addressing the user's question directly, then skip to Step 6. \n",
    "Original answer: {original_answer} \n",
    "4. Subtly integrate follow-up questions or statements into the paraphrased answer to guide the conversation towards providing information to fill in the 'None' values in the eligibility table. The paraphrased response should only refer to one such value at a time. Ensure that the paraphrased response addresses the given question as the primary focus, and guiding the conversation being secondary. Maintain the core topic and intent of the original answer throughout the paraphrasing process.\n",
    "5. Ensure guidance provided aligns with the natural flow of the conversation, and does not explicitly reveal your intention to assess eligibility, or your decision-making process based on the eligibility table. \n",
    "6. Store the returned response in memory:\n",
    "{{paraphrased_response}}\n",
    "7. Analyse the final paraphrased response to identify potential areas where the user might want to find out more information, including key topics or examples mentioned. \n",
    "8. Generate a minimum of 2 and maximum of 4 possible replies or questions from the perspective of the human, that they might might make to your paraphrased response. Ensure that each option is concise (<124 characters) and avoids derogatory language.\n",
    "9. Where relevant, provide options that allow the user to: Clarify or understand more about each specific topic or example mentioned in the paraphrased response, skip answering the question where necessary, engage in further discussion or seek additional information. \n",
    "10. Maintain coherence between the paraphrased response and the generated reply options to facilitate a smooth transition in the conversation.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0967ea7a-3f4a-4f24-936f-7dc19ab3ad28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_core.runnables import RunnableParallel, RunnableSequence\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class ParaphraseAndSuggestOutput(BaseModel):\n",
    "    \"\"\"\n",
    "    Typings for paraphrased chain output\n",
    "    \"\"\"\n",
    "    answer: str = Field(description=\"answer\")\n",
    "    reply_options: List[str] = Field(\n",
    "        description=\"list of suggested user responses\")\n",
    "\n",
    "\n",
    "class CompositeChain(metaclass=SingletonMeta):\n",
    "    \"\"\"\n",
    "    This class combines both evaluation_chain and conversation_retrieval_chain \n",
    "    and paraphrases the answer if necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"This method instantiates an instance of CompositeChain\"\"\"\n",
    "        self.eligibility_dict = {\n",
    "            criteria.name: None\n",
    "            for criteria in CriteriaEnum\n",
    "        }\n",
    "        self.eval_chain = EvaluationChain(self.eligibility_dict).chain\n",
    "        self.cr_chain = ConversationalRetrievalChain().chain\n",
    "        self.paraphrase_and_suggest_chain = self.get_paraphrase_and_suggest_chain()\n",
    "        self.chain = self.get_chain()\n",
    "\n",
    "    def get_chain(self) -> RunnableSequence:\n",
    "        \"\"\"\n",
    "        This method returns the composite chain.\n",
    "\n",
    "        ..composite_chain\n",
    "          (evaluation_chain, conversational_chain) | paraphrase_chain\n",
    "\n",
    "        The evaluation_chain and classifier_chain are ran in parallel. The outputs are passed\n",
    "        to the paraphrase_chain which paraphrases the answer from classifier_chain based on\n",
    "        the output from evaluation_chain.\n",
    "\n",
    "        This is to guide the conversation along to determine the eligibility of the user for\n",
    "        the grants.\n",
    "        \"\"\"\n",
    "        chain = (\n",
    "            RunnableParallel(\n",
    "                {\n",
    "                    \"eval_chain\": self.eval_chain,\n",
    "                    \"conversational_chain\": self.cr_chain,\n",
    "                })\n",
    "            | {\n",
    "                \"response\": self.paraphrase_and_suggest_chain,\n",
    "                \"information\": lambda x: x[\"conversational_chain\"][\"information\"] if \"information\" in x[\"conversational_chain\"] else [],\n",
    "                \"topic\": lambda x: x[\"conversational_chain\"][\"topic\"]\n",
    "            }\n",
    "            | {\n",
    "                \"answer\": lambda x: x['response'][\"answer\"],\n",
    "                \"reply_options\": lambda x: x['response'][\"reply_options\"],\n",
    "                \"information\": lambda x: x[\"information\"],\n",
    "                \"topic\": lambda x: x[\"topic\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return chain\n",
    "\n",
    "    def get_paraphrase_and_suggest_chain(self) -> RunnableSequence:\n",
    "        \"\"\"This method returns a chain that paraphrases the answer, and provides suggested human responses\"\"\"\n",
    "        parser = JsonOutputParser(pydantic_object=ParaphraseAndSuggestOutput)\n",
    "        PARAPHRASE_AND_SUGGEST_PROMPT = PromptTemplate.from_template(PARAPHRASE_AND_SUGGEST_TEMPLATE, partial_variables={\n",
    "            \"format_instructions\": parser.get_format_instructions()\n",
    "        })\n",
    "\n",
    "        paraphrase_and_suggest_chain = (\n",
    "            {\n",
    "                \"original_answer\": lambda x: x[\"conversational_chain\"][\"answer\"],\n",
    "                \"eligibility_table\": lambda x: self.eligibility_dict,\n",
    "                \"question\": lambda x: x[\"conversational_chain\"][\"question\"]\n",
    "            }\n",
    "            | PARAPHRASE_AND_SUGGEST_PROMPT\n",
    "            | ChatVertexAI(model_name=\"chat-bison-32k\", temperature=0, verbose=True, max_output_tokens=8192)\n",
    "            | parser\n",
    "        )\n",
    "\n",
    "        return paraphrase_and_suggest_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c81be-8b65-43a5-93ae-678860a6dc26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain().chain\n",
    "\n",
    "question = \"Tell me about Home Caregiving Grant.\"\n",
    "\n",
    "result = chain.invoke({ \"question\": question })\n",
    "\n",
    "print(\"======= answer ========\", end=\"\\n\")\n",
    "pp.pprint(result)\n",
    "\n",
    "# print(\"======= docs ========\", end=\"\\n\")\n",
    "# for i,doc in enumerate(result[\"docs\"]):\n",
    "#     pp.pprint(f\"{i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5804a31-dc6c-4d02-8843-3a7f98a515db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Can you help me assess my eligibility?\"\n",
    "res = chain.invoke({\"question\": question})\n",
    "pp.pprint(res)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3-11-6",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": "python3-11-6 (Local)",
   "language": "python",
   "name": "python3-11-6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
