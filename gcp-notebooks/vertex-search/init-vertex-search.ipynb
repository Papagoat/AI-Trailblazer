{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a878273f-8bea-416d-92e3-e01d9139b563",
   "metadata": {},
   "source": [
    "# STEP 1: Create Matching Engine Index and Endpoint for Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6b17f-e958-49f4-aa62-085bb9f31e4f",
   "metadata": {},
   "source": [
    "Configure parameters to create Matching Engine index\n",
    "- ME_REGION: Region where Matching Engine Index and Index Endpoint are deployed\n",
    "- ME_INDEX_NAME: Matching Engine index display name\n",
    "- ME_EMBEDDING_DIR: Cloud Storage path to allow inserting, updating or deleting the contents of the Index\n",
    "- STEP 1: Create Matching Engine Index and Endpoint for Retrieval\n",
    "ME_DIMENSIONS: The number of dimensions of the input vectors. Vertex AI Embedding API generates 768 dimensional vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2eecf-84bf-4616-8eb2-d51786d006d7",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a537a9-1db8-4ae2-b780-61057d727ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform==1.35.0\n",
      "  Downloading google_cloud_aiplatform-1.35.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting langchain==0.0.323\n",
      "  Downloading langchain-0.0.323-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (3.19.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (23.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (3.13.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (1.11.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (2.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (3.9.1)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.323)\n",
      "  Downloading dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (1.33)\n",
      "Collecting langsmith<0.1.0,>=0.0.43 (from langchain==0.0.323)\n",
      "  Downloading langsmith-0.0.69-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.323) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.323) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.323) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.323) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.323) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.323) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.323) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.323) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.323) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.323)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.323)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (2.25.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.48.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.48.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.35.0) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.323) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.323) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.323) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.323) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.323) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.323) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (4.9)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.16.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.323)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (0.5.1)\n",
      "Downloading google_cloud_aiplatform-1.35.0-py2.py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.0.323-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain, google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script langsmith is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts langchain and langchain-server are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.3 google-cloud-aiplatform-1.35.0 langchain-0.0.323 langsmith-0.0.69 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "The following additional packages will be installed:\n",
      "  libarchive-dev liblept5 libleptonica-dev libopenjp2-7 libtesseract4\n",
      "  libwebpmux3 tesseract-ocr-eng tesseract-ocr-osd\n",
      "The following NEW packages will be installed:\n",
      "  libarchive-dev liblept5 libleptonica-dev libopenjp2-7 libtesseract-dev\n",
      "  libtesseract4 libwebpmux3 tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
      "0 upgraded, 10 newly installed, 0 to remove and 2 not upgraded.\n",
      "Need to get 10.9 MB of archives.\n",
      "After this operation, 38.9 MB of additional disk space will be used.\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libarchive-dev:amd64.\n",
      "(Reading database ... 131268 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libarchive-dev_3.4.3-2+deb11u1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libarchive-dev:amd64 (3.4.3-2+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libopenjp2-7:amd64.\n",
      "Preparing to unpack .../1-libopenjp2-7_2.4.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libopenjp2-7:amd64 (2.4.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libwebpmux3:amd64.\n",
      "Preparing to unpack .../2-libwebpmux3_0.6.1-2.1+deb11u2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libwebpmux3:amd64 (0.6.1-2.1+deb11u2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package liblept5:amd64.\n",
      "Preparing to unpack .../3-liblept5_1.79.0-1.1+deb11u1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking liblept5:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libleptonica-dev:amd64.\n",
      "Preparing to unpack .../4-libleptonica-dev_1.79.0-1.1+deb11u1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libleptonica-dev:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libtesseract4:amd64.\n",
      "Preparing to unpack .../5-libtesseract4_4.1.1-2.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libtesseract4:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libtesseract-dev:amd64.\n",
      "Preparing to unpack .../6-libtesseract-dev_4.1.1-2.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libtesseract-dev:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package tesseract-ocr-eng.\n",
      "Preparing to unpack .../7-tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package tesseract-ocr-osd.\n",
      "Preparing to unpack .../8-tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package tesseract-ocr.\n",
      "Preparing to unpack .../9-tesseract-ocr_4.1.1-2.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking tesseract-ocr (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libarchive-dev:amd64 (3.4.3-2+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libopenjp2-7:amd64 (2.4.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libwebpmux3:amd64 (0.6.1-2.1+deb11u2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up liblept5:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libleptonica-dev:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libtesseract4:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libtesseract-dev:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up tesseract-ocr (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for man-db (2.9.4-2) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u7) ...\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvonnxparser.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_plugin.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvparsers.so.8 is not a symbolic link\n",
      "\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JSelecting previously unselected package poppler-data.\n",
      "(Reading database ... 131510 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-data_0.4.10-1_all.deb ...\n",
      "Unpacking poppler-data (0.4.10-1) ...\n",
      "Selecting previously unselected package libnspr4:amd64.\n",
      "Preparing to unpack .../libnspr4_2%3a4.29-1_amd64.deb ...\n",
      "Unpacking libnspr4:amd64 (2:4.29-1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../libnss3_2%3a3.61-1+deb11u3_amd64.deb ...\n",
      "Unpacking libnss3:amd64 (2:3.61-1+deb11u3) ...\n",
      "Selecting previously unselected package libpoppler102:amd64.\n",
      "Preparing to unpack .../libpoppler102_20.09.0-3.1+deb11u1_amd64.deb ...\n",
      "Unpacking libpoppler102:amd64 (20.09.0-3.1+deb11u1) ...\n",
      "Selecting previously unselected package poppler-utils.\n",
      "Preparing to unpack .../poppler-utils_20.09.0-3.1+deb11u1_amd64.deb ...\n",
      "Unpacking poppler-utils (20.09.0-3.1+deb11u1) ...\n",
      "Setting up poppler-data (0.4.10-1) ...\n",
      "Setting up libnspr4:amd64 (2:4.29-1) ...\n",
      "Setting up libnss3:amd64 (2:3.61-1+deb11u3) ...\n",
      "Setting up libpoppler102:amd64 (20.09.0-3.1+deb11u1) ...\n",
      "Setting up poppler-utils (20.09.0-3.1+deb11u1) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u7) ...\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvonnxparser.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_plugin.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvparsers.so.8 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.9.4-2) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2) ...\n",
      "Collecting unstructured==0.7.5\n",
      "  Downloading unstructured-0.7.5-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pdf2image==1.16.3\n",
      "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
      "Collecting pytesseract==0.3.10\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Collecting pdfminer.six==20221105\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting argilla (from unstructured==0.7.5)\n",
      "  Downloading argilla-1.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting chardet (from unstructured==0.7.5)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured==0.7.5)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting lxml (from unstructured==0.7.5)\n",
      "  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting msg-parser (from unstructured==0.7.5)\n",
      "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk (from unstructured==0.7.5)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openpyxl (from unstructured==0.7.5)\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from unstructured==0.7.5) (2.0.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from unstructured==0.7.5) (10.1.0)\n",
      "Collecting pypandoc (from unstructured==0.7.5)\n",
      "  Downloading pypandoc-1.12-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-docx (from unstructured==0.7.5)\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx (from unstructured==0.7.5)\n",
      "  Downloading python_pptx-0.6.23-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting python-magic (from unstructured==0.7.5)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from unstructured==0.7.5) (3.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from unstructured==0.7.5) (2.31.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured==0.7.5) (0.9.0)\n",
      "Collecting xlrd (from unstructured==0.7.5)\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from pytesseract==0.3.10) (23.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105) (41.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.16.0)\n",
      "Collecting httpx<=0.25,>=0.15 (from argilla->unstructured==0.7.5)\n",
      "  Downloading httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: deprecated~=1.2.0 in /opt/conda/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (1.2.14)\n",
      "Collecting pandas (from unstructured==0.7.5)\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2.0,>=1.10.7 in /opt/conda/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (1.10.13)\n",
      "Collecting wrapt<1.15,>=1.13 (from argilla->unstructured==0.7.5)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.24.0 (from argilla->unstructured==0.7.5)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (4.66.1)\n",
      "Requirement already satisfied: backoff in /opt/conda/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (2.2.1)\n",
      "Collecting monotonic (from argilla->unstructured==0.7.5)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: rich!=13.1.0 in /opt/conda/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (13.7.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->unstructured==0.7.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->unstructured==0.7.5) (2023.3.post1)\n",
      "Collecting olefile>=0.46 (from msg-parser->unstructured==0.7.5)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.7.5) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.7.5) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.7.5) (2023.10.3)\n",
      "Collecting et-xmlfile (from openpyxl->unstructured==0.7.5)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from python-docx->unstructured==0.7.5) (4.9.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx->unstructured==0.7.5)\n",
      "  Downloading XlsxWriter-3.1.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured==0.7.5) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured==0.7.5) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured==0.7.5) (2023.11.17)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105) (2.21)\n",
      "Collecting httpcore<0.19.0,>=0.18.0 (from httpx<=0.25,>=0.15->argilla->unstructured==0.7.5)\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<=0.25,>=0.15->argilla->unstructured==0.7.5) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->unstructured==0.7.5) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich!=13.1.0->argilla->unstructured==0.7.5) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich!=13.1.0->argilla->unstructured==0.7.5) (2.17.2)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.19.0,>=0.18.0->httpx<=0.25,>=0.15->argilla->unstructured==0.7.5) (3.7.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.19.0,>=0.18.0->httpx<=0.25,>=0.15->argilla->unstructured==0.7.5) (0.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich!=13.1.0->argilla->unstructured==0.7.5) (0.1.2)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx<=0.25,>=0.15->argilla->unstructured==0.7.5) (1.2.0)\n",
      "Downloading unstructured-0.7.5-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading argilla-1.20.0-py3-none-any.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypandoc-1.12-py3-none-any.whl (20 kB)\n",
      "Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: monotonic, filetype, XlsxWriter, xlrd, wrapt, python-magic, pytesseract, pypandoc, pdf2image, olefile, numpy, nltk, lxml, et-xmlfile, chardet, python-pptx, python-docx, pandas, openpyxl, msg-parser, httpcore, pdfminer.six, httpx, argilla, unstructured\n",
      "\u001b[33m  WARNING: The script filetype is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pytesseract is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script chardetect is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script msg_parser is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script argilla is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script unstructured-ingest is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed XlsxWriter-3.1.9 argilla-1.20.0 chardet-5.2.0 et-xmlfile-1.1.0 filetype-1.2.0 httpcore-0.18.0 httpx-0.25.0 lxml-4.9.3 monotonic-1.6 msg-parser-1.2.0 nltk-3.8.1 numpy-1.23.5 olefile-0.47 openpyxl-3.1.2 pandas-1.5.3 pdf2image-1.16.3 pdfminer.six-20221105 pypandoc-1.12 pytesseract-0.3.10 python-docx-1.1.0 python-magic-0.4.27 python-pptx-0.6.23 unstructured-0.7.5 wrapt-1.14.1 xlrd-2.0.1\n",
      "Collecting tensorflow_hub==0.13.0\n",
      "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow_text==2.12.1\n",
      "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in ./.local/lib/python3.10/site-packages (from tensorflow_hub==0.13.0) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow_hub==0.13.0) (3.19.6)\n",
      "Collecting tensorflow<2.13,>=2.12.0 (from tensorflow_text==2.12.1)\n",
      "  Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.48.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.10.0)\n",
      "Collecting jax>=0.3.15 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading jax-0.4.21-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (23.2)\n",
      "Collecting protobuf>=3.19.6 (from tensorflow_hub==0.13.0)\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.16.0)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.4.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.29.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.42.0)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.11.4)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.25.2)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.1.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2023.11.17)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.2.2)\n",
      "Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m165.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jax-0.4.21-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, tensorflow-estimator, tensorboard-data-server, protobuf, ml-dtypes, keras, grpcio, tensorflow_hub, jax, google-auth-oauthlib, tensorboard, tensorflow, tensorflow_text\n",
      "\u001b[33m  WARNING: The scripts make_image_classifier and make_nearest_neighbour_index are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.1 which is incompatible.\n",
      "fastapi 0.104.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "google-api-core 1.34.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.1 which is incompatible.\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.1 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.1 which is incompatible.\n",
      "google-cloud-language 1.3.2 requires protobuf<4.0.0dev, but you have protobuf 4.25.1 which is incompatible.\n",
      "google-cloud-videointelligence 1.16.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.1 which is incompatible.\n",
      "kfp 2.4.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\n",
      "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.25.1 which is incompatible.\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.1 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.1 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.1 which is incompatible.\n",
      "typeguard 4.1.5 requires typing-extensions>=4.7.0; python_version < \"3.12\", but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 grpcio-1.60.0 jax-0.4.21 keras-2.12.0 ml-dtypes-0.3.1 protobuf-4.25.1 tensorboard-2.12.3 tensorboard-data-server-0.7.2 tensorflow-2.12.1 tensorflow-estimator-2.12.0 tensorflow_hub-0.13.0 tensorflow_text-2.12.1 typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "# Install Vertex AI LLM SDK\n",
    "! pip install --user --upgrade google-cloud-aiplatform==1.35.0 langchain==0.0.323\n",
    "\n",
    "# Dependencies required by Unstructured PDF loader\n",
    "! sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
    "! sudo apt-get -y -qq install poppler-utils\n",
    "! pip install --user unstructured==0.7.5 pdf2image==1.16.3 pytesseract==0.3.10 pdfminer.six==20221105\n",
    "\n",
    "# For Matching Engine integration dependencies (default embeddings)\n",
    "! pip install --user tensorflow_hub==0.13.0 tensorflow_text==2.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e5ef3-ecf8-4c6d-8199-469aba3c4a4d",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d2987c-08ef-4c5f-8a17-82354f1682b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76451ae4-b442-425f-9964-40977aa5e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"utils\"):\n",
    "    os.makedirs(\"utils\")\n",
    "\n",
    "url_prefix = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/use-cases/document-qa/utils\"\n",
    "files = [\"__init__.py\", \"matching_engine.py\", \"matching_engine_utils.py\"]\n",
    "\n",
    "for fname in files:\n",
    "    urllib.request.urlretrieve(f\"{url_prefix}/{fname}\", filename=f\"utils/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de65e13-4534-4c1b-96ef-c1d19388ccfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.0.323\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import langchain\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils\n",
    "from langchain.document_loaders import GCSDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d46e04-0d43-4b01-8b30-31c3a0b5209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"engaged-domain-403109\"  # @param {type:\"string\"}\n",
    "REGION = \"asia-southeast1\"  # @param {type:\"string\"}\n",
    "\n",
    "ME_REGION = REGION\n",
    "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"  # @param {type:\"string\"}\n",
    "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"  # @param {type:\"string\"}\n",
    "ME_DIMENSIONS = 768  # when using Vertex PaLM Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d1a9c-ca01-486e-afcc-f558a7bfe0a9",
   "metadata": {},
   "source": [
    "## Make a Google Cloud Storage bucket for your Matching Engine index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc640c6-2411-4efc-ae86-34d5814d9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ gsutil mb -p engaged-domain-403109 -l asia-southeast1 gs://engaged-domain-403109-me-bucket\n",
      "Creating gs://engaged-domain-403109-me-bucket/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'engaged-domain-403109-me-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! set -x && gsutil mb -p $PROJECT_ID -l $REGION gs://$ME_EMBEDDING_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aadf3b-eea5-4b44-a9c4-115cad335faf",
   "metadata": {},
   "source": [
    "### Create a dummy embeddings file to initialize when creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfeb9c7-4af1-4aa9-869b-603e71e76f83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ gsutil cp embeddings_0.json gs://engaged-domain-403109-me-bucket/init_index/embeddings_0.json\n",
      "Copying file://embeddings_0.json [Content-Type=application/json]...\n",
      "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 1 objects/3.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# dummy embedding\n",
    "init_embedding = {\"id\": str(uuid.uuid4()), \"embedding\": list(np.zeros(ME_DIMENSIONS))}\n",
    "\n",
    "# dump embedding to a local file\n",
    "with open(\"embeddings_0.json\", \"w\") as f:\n",
    "    json.dump(init_embedding, f)\n",
    "\n",
    "# write embedding to Cloud Storage\n",
    "! set -x && gsutil cp embeddings_0.json gs://{ME_EMBEDDING_DIR}/init_index/embeddings_0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9527d7-85e3-49c1-879a-b013db117bfe",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77b0c14-4292-4d5e-bba6-7d8bddab98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mengine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3c927e-2acd-4573-977c-e39a8c4f945d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mengine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mmengine\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_index(\n\u001b[1;32m      2\u001b[0m     embedding_gcs_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mME_EMBEDDING_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/init_index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     dimensions\u001b[38;5;241m=\u001b[39mME_DIMENSIONS,\n\u001b[1;32m      4\u001b[0m     index_update_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# can change to batch updates if needed\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     index_algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree-ah\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mengine' is not defined"
     ]
    }
   ],
   "source": [
    "index = mengine.create_index(\n",
    "    embedding_gcs_uri=f\"gs://{ME_EMBEDDING_DIR}/init_index\",\n",
    "    dimensions=ME_DIMENSIONS,\n",
    "    index_update_method=\"streaming\", # can change to batch updates if needed\n",
    "    index_algorithm=\"tree-ah\",\n",
    ")\n",
    "if index:\n",
    "    print(index.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484e1ef-64ad-4cc1-adf1-fbf30a651f7f",
   "metadata": {},
   "source": [
    "## Deploy Index to Endpoint\n",
    "Deploy index to Index Endpoint on Matching Engine. This notebook deploys the index to a public endpoint. The deployment operation creates a public endpoint that will be used for querying the index for approximate nearest neighbors.\n",
    "\n",
    "For deploying index to a Private Endpoint, refer to the documentation to set up pre-requisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb013acc-55e3-4514-9acf-3a79147eb0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Index endpoint engaged-domain-403109-me-index-endpoint does not exists. Creating index endpoint...\n",
      "INFO:root:Deploying index to endpoint with long running operation projects/510519063638/locations/asia-southeast1/indexEndpoints/3617586769429528576/operations/5185750934893887488\n",
      "INFO:root:Poll the operation to create index endpoint ...\n",
      "INFO:root:Index endpoint engaged-domain-403109-me-index-endpoint created with resource name as projects/510519063638/locations/asia-southeast1/indexEndpoints/3617586769429528576 and endpoint domain name as \n",
      "INFO:root:Deploying index with request = {'id': 'engaged_domain_403109_me_index_20231213104529', 'display_name': 'engaged_domain_403109_me_index_20231213104529', 'index': 'projects/510519063638/locations/asia-southeast1/indexes/4693366538231611392', 'dedicated_resources': {'machine_spec': {'machine_type': 'e2-standard-2'}, 'min_replica_count': 2, 'max_replica_count': 10}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Poll the operation to deploy index ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deployed index engaged-domain-403109-me-index to endpoint engaged-domain-403109-me-index-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Index endpoint resource name: projects/510519063638/locations/asia-southeast1/indexEndpoints/3617586769429528576\n",
      "Index endpoint public domain name: \n",
      "Deployed indexes on the index endpoint:\n"
     ]
    }
   ],
   "source": [
    "index_endpoint = mengine.deploy_index()\n",
    "if index_endpoint:\n",
    "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "    print(\n",
    "        f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\"\n",
    "    )\n",
    "    print(\"Deployed indexes on the index endpoint:\")\n",
    "    for d in index_endpoint.deployed_indexes:\n",
    "        print(f\"    {d.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aadb60-783e-489f-b085-33ac315744a9",
   "metadata": {},
   "source": [
    "# STEP 2: Add Document Embeddings to Matching Engine - Vector Store\n",
    "This step ingests and parse PDF documents, split them, generate embeddings and add the embeddings to the vector store. The document corpus used as dataset is a sample of Google published research papers across different domains - large models, traffic simulation, productivity etc.\n",
    "\n",
    "## Ingest PDF files\n",
    "The document corpus is hosted on Cloud Storage bucket (at gs://github-repo/documents/google-research-pdfs/) and LangChain provides a convenient document loader GCSDirectoryLoader to load documents from a Cloud Storage bucket. The loader uses Unstructured package to load files of many types including pdfs, images, html and more.\n",
    "\n",
    "Make a Google Cloud Storage bucket in your GCP project to copy the document files into.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253830a-0701-4860-b57e-eaf0cdcd7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! set -x && gsutil mb -p $PROJECT_ID -l us-central1 gs://$GCS_BUCKET_DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da7b559-cadc-4828-b2b1-231a6ea9cdab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCS_BUCKET_DOCS = \"financial-websites-pdfs\"\n",
    "folder_prefix = \"pdfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6a28be-59b8-45f5-9800-ddd81660cb67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents from financial-websites-pdfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents loaded (pre-chunking) = 19\n"
     ]
    }
   ],
   "source": [
    "# Load documents and add document metadata such as file name, to be retrieved later when citing the references.\n",
    "\n",
    "# Ingest PDF files\n",
    "\n",
    "print(f\"Processing documents from {GCS_BUCKET_DOCS}\")\n",
    "loader = GCSDirectoryLoader(\n",
    "    project_name=PROJECT_ID, bucket=GCS_BUCKET_DOCS, prefix=folder_prefix\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# Add document name and source to the metadata\n",
    "for document in documents:\n",
    "    doc_md = document.metadata\n",
    "    document_name = doc_md[\"source\"].split(\"/\")[-1]\n",
    "    # derive doc source from Document loader\n",
    "    doc_source_prefix = \"/\".join(GCS_BUCKET_DOCS.split(\"/\")[:3])\n",
    "    doc_source_suffix = \"/\".join(doc_md[\"source\"].split(\"/\")[4:-1])\n",
    "    source = f\"{doc_source_prefix}/{doc_source_suffix}\"\n",
    "    document.metadata = {\"source\": source, \"document_name\": document_name}\n",
    "\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b7066e-cd8c-4f03-ab45-a247ba1f9b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'financial-websites-pdfs/',\n",
       " 'document_name': 'annual value as of 2022 for social support schemes.pdf'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify document metadata\n",
    "\n",
    "documents[0].metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca56a0-d4fc-4449-bbf3-a273d671c02e",
   "metadata": {},
   "source": [
    "## Chunk documents\n",
    "Split the documents to smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c4c6c5-f38a-4f24-9e4e-6c1ab1aab9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 195\n"
     ]
    }
   ],
   "source": [
    "# split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add chunk number to metadata\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "print(f\"# of documents = {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47032f5-c567-420f-8cab-01406998a077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'financial-websites-pdfs/',\n",
       " 'document_name': 'annual value as of 2022 for social support schemes.pdf',\n",
       " 'chunk': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits[0].metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616413ff-f34b-4115-ba00-1445531f616f",
   "metadata": {},
   "source": [
    "## Configure Matching Engine as Vector Store\n",
    "\n",
    "### Get Matching Engine Index id and Endpoint id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1460201d-f4a8-4ce3-b10d-87dfc3ca7412",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ME_INDEX_ID=projects/510519063638/locations/asia-southeast1/indexes/4693366538231611392\n",
      "ME_INDEX_ENDPOINT_ID=projects/510519063638/locations/asia-southeast1/indexEndpoints/3617586769429528576\n"
     ]
    }
   ],
   "source": [
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
    "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
    "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ac586-3f40-4e3f-903f-67e922f15336",
   "metadata": {},
   "source": [
    "### Next you will define some utility functions that you will use for the Vertex AI Embeddings API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b8b3f2-ce12-4282-8741-b957c1ed574e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6119f7-958f-4d59-a121-f7c1dc2f91ed",
   "metadata": {},
   "source": [
    "### Initialize Matching Engine vector store with text embeddings model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c84be6-b164-4e62-9796-4258c7339726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 13:24:52.477684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ME_INDEX_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m CustomVertexAIEmbeddings(\n\u001b[1;32m      5\u001b[0m     requests_per_minute\u001b[38;5;241m=\u001b[39mEMBEDDING_QPM,\n\u001b[1;32m      6\u001b[0m     num_instances_per_batch\u001b[38;5;241m=\u001b[39mEMBEDDING_NUM_BATCH,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# initialize vector store\u001b[39;00m\n\u001b[1;32m     10\u001b[0m me \u001b[38;5;241m=\u001b[39m MatchingEngine\u001b[38;5;241m.\u001b[39mfrom_components(\n\u001b[1;32m     11\u001b[0m     project_id\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[1;32m     12\u001b[0m     region\u001b[38;5;241m=\u001b[39mME_REGION,\n\u001b[1;32m     13\u001b[0m     gcs_bucket_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mME_EMBEDDING_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     14\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m---> 15\u001b[0m     index_id\u001b[38;5;241m=\u001b[39m\u001b[43mME_INDEX_ID\u001b[49m,\n\u001b[1;32m     16\u001b[0m     endpoint_id\u001b[38;5;241m=\u001b[39mME_INDEX_ENDPOINT_ID,\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ME_INDEX_ID' is not defined"
     ]
    }
   ],
   "source": [
    "# Embeddings API integrated with langChain\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")\n",
    "\n",
    "# initialize vector store\n",
    "me = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=ME_REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfca82c-32b1-4ad4-b252-04a85701947b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add documents as embeddings in Matching Engine as index\n",
    "\n",
    "The document chunks are transformed as embeddings (vectors) using Vertex AI Embeddings API and added to the index with streaming index update. With Streaming Updates, you can update and query your index within a few seconds.\n",
    "\n",
    "The original document text is stored on Cloud Storage bucket had referenced by id.\n",
    "\n",
    "Prepare text and metadata to be added to the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "415f0805-ccc2-47e9-9af1-1c0cfc8bbfe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store docs as embeddings in Matching Engine index\n",
    "# It may take a while since API is rate limited\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [\n",
    "    [\n",
    "        {\"namespace\": \"source\", \"allow_list\": [doc.metadata[\"source\"]]},\n",
    "        {\"namespace\": \"document_name\", \"allow_list\": [doc.metadata[\"document_name\"]]},\n",
    "        {\"namespace\": \"chunk\", \"allow_list\": [str(doc.metadata[\"chunk\"])]},\n",
    "    ]\n",
    "    for doc in doc_splits\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76e4b2-21f8-42fe-9a54-2b72f97d10a1",
   "metadata": {},
   "source": [
    "### Add embeddings to the vector store\n",
    "\n",
    "NOTE: Depending on the volume and size of documents, this step may take time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "374b0cf4-e294-4580-bc41-da2a444329e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n",
      ".................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indexed 195 documents to Matching Engine.\n"
     ]
    }
   ],
   "source": [
    "doc_ids = me.add_texts(texts=texts, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134192d7-60d5-4cd7-a906-5bcb6f205d52",
   "metadata": {},
   "source": [
    "## Validate semantic search with Matching Engine is working\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c60e1270-8fd2-43c2-8e97-430a122d8591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Special physical facilities which may include sensory modulation rooms, vocational training rooms, depending on the needs\\n\\nof their students\\n\\nConsiderations when selecting a SPED school:\\n\\n6 Additionally, in selecting a SPED school for your child, you can consider these other factors :\\n\\nYour child’s needs and education pathways\\n\\nDistance from home to school – A nearer school means reduced transport costs and shorter travelling time\\n\\nYour child’s interest and whether the school o\\x00ers CCAs and activities that matches these interests; and\\n\\nSchool identity, including the school’s vision, mission, culture.\\n\\nDepending on your child's abilities, you can enrol your child into di\\x00erent types of SPED schools:\\n\\nThose that follow the National Curriculum (e.g. Pathlight School)\\n\\nYour child will need to have adequate cognitive and adaptive skills to keep up with the mainstream curriculum Your child will receive support in daily living and social-emotional skills\", metadata={'source': 'financial-websites-pdfs/', 'document_name': 'disability support - education.pdf', 'chunk': '49', 'score': 0.7982342839241028}),\n",
       " Document(page_content='The Learning Support Plan (LSP) and appropriate strategies for your child. Support arrangements for your child (e.g. the school can allocate a \\x00rst-level classroom for your child if they face mobility challenges)\\n\\nThe school’s professionals can help in assessing the needs of your child and recommending adjustments to be made.\\n\\nForeign System Schools (FSS)\\n\\nSome FSS - also known as \"international schools\" - cater to students with special education needs. However, as these schools are intended primarily for expatriates\\' children, Singaporeans who wish to enrol in FSS (excluding preschools) will need to seek approval from MOE. Learning disabilities may be a factor of consideration in the approval process.\\n\\nSpecial education\\n\\nActions to take\\n\\nTake note that di\\x00erent SPED schools cater to students with di\\x00erent disabilities.\\n\\nFind out more about special education on the MOE website and in this information guide to SPED schools in Singapore.', metadata={'source': 'financial-websites-pdfs/', 'document_name': 'disability support - education.pdf', 'chunk': '47', 'score': 0.7678547501564026})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test whether search from vector store is working\n",
    "me.similarity_search(\"What are the criterias to go to schools for SPED?\", k=2)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
